{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-22T13:33:30.996833Z",
     "iopub.status.busy": "2025-05-22T13:33:30.996206Z",
     "iopub.status.idle": "2025-05-22T13:33:31.343317Z",
     "shell.execute_reply": "2025-05-22T13:33:31.342558Z",
     "shell.execute_reply.started": "2025-05-22T13:33:30.996810Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/dataset/dakshina_dataset_v1.0/README.md\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv/wiki-full.urls.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv/wiki-full.urls.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv/wiki-full.urls.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv/wiki-full.urls.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv/wiki-full.urls.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv/wiki-full.urls.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv/wiki-full.urls.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv/wiki-full.urls.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv/wiki-full.urls.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv/wiki-full.urls.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv/wiki-full.urls.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv/wiki-full.urls.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
      "/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:33:35.146383Z",
     "iopub.status.busy": "2025-05-22T13:33:35.145893Z",
     "iopub.status.idle": "2025-05-22T13:33:35.152022Z",
     "shell.execute_reply": "2025-05-22T13:33:35.151067Z",
     "shell.execute_reply.started": "2025-05-22T13:33:35.146360Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "import logging\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:33:40.966539Z",
     "iopub.status.busy": "2025-05-22T13:33:40.966296Z",
     "iopub.status.idle": "2025-05-22T13:33:40.972363Z",
     "shell.execute_reply": "2025-05-22T13:33:40.971842Z",
     "shell.execute_reply.started": "2025-05-22T13:33:40.966522Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    }
   ],
   "source": [
    "wandb.login(key=\"367b223a3f0954311018074733b3ed8dc409c2ff\")\n",
    "# Constants\n",
    "wandb_project = \"DL_A3_seq2seq\"\n",
    "wandb_entity= \"da24m006-iit-madras\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:33:47.793443Z",
     "iopub.status.busy": "2025-05-22T13:33:47.792803Z",
     "iopub.status.idle": "2025-05-22T13:33:47.799217Z",
     "shell.execute_reply": "2025-05-22T13:33:47.798631Z",
     "shell.execute_reply.started": "2025-05-22T13:33:47.793420Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:33:50.866133Z",
     "iopub.status.busy": "2025-05-22T13:33:50.865868Z",
     "iopub.status.idle": "2025-05-22T13:33:50.941756Z",
     "shell.execute_reply": "2025-05-22T13:33:50.941190Z",
     "shell.execute_reply.started": "2025-05-22T13:33:50.866113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons'\n",
    "traindata = pd.read_csv(f'{data_dir}/hi.translit.sampled.train.tsv',sep='\\t',names=['native', 'latin', 'count'],usecols=[0, 1])\n",
    "valdata= pd.read_csv(f'{data_dir}/hi.translit.sampled.dev.tsv',sep='\\t',names=['native', 'latin', 'count'],usecols=[0, 1])\n",
    "testdata = pd.read_csv(f'{data_dir}/hi.translit.sampled.test.tsv',sep='\\t',names=['native', 'latin', 'count'],usecols=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:33:53.045736Z",
     "iopub.status.busy": "2025-05-22T13:33:53.044992Z",
     "iopub.status.idle": "2025-05-22T13:33:53.055444Z",
     "shell.execute_reply": "2025-05-22T13:33:53.054745Z",
     "shell.execute_reply.started": "2025-05-22T13:33:53.045700Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>native</th>\n",
       "      <th>latin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>अं</td>\n",
       "      <td>an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>अंकगणित</td>\n",
       "      <td>ankganit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>अंकल</td>\n",
       "      <td>uncle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>अंकुर</td>\n",
       "      <td>ankur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>अंकुरण</td>\n",
       "      <td>ankuran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44199</th>\n",
       "      <td>ह्वेनसांग</td>\n",
       "      <td>hiuentsang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44200</th>\n",
       "      <td>ह्वेनसांग</td>\n",
       "      <td>hsuantsang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44201</th>\n",
       "      <td>ह्वेनसांग</td>\n",
       "      <td>hyensang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44202</th>\n",
       "      <td>ह्वेनसांग</td>\n",
       "      <td>xuanzang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44203</th>\n",
       "      <td>ॐ</td>\n",
       "      <td>om</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44204 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          native       latin\n",
       "0             अं          an\n",
       "1        अंकगणित    ankganit\n",
       "2           अंकल       uncle\n",
       "3          अंकुर       ankur\n",
       "4         अंकुरण     ankuran\n",
       "...          ...         ...\n",
       "44199  ह्वेनसांग  hiuentsang\n",
       "44200  ह्वेनसांग  hsuantsang\n",
       "44201  ह्वेनसांग    hyensang\n",
       "44202  ह्वेनसांग    xuanzang\n",
       "44203          ॐ          om\n",
       "\n",
       "[44204 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:33:58.310213Z",
     "iopub.status.busy": "2025-05-22T13:33:58.309948Z",
     "iopub.status.idle": "2025-05-22T13:33:58.319289Z",
     "shell.execute_reply": "2025-05-22T13:33:58.318552Z",
     "shell.execute_reply.started": "2025-05-22T13:33:58.310194Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dataset: \n",
      "Total train data: 44204\n",
      "Total validation data: 4358\n",
      "Totat test data: 4502\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Dataset: \")\n",
    "print(\"Total train data:\", len(traindata))\n",
    "print(\"Total validation data:\", len(valdata))\n",
    "print(\"Totat test data:\", len(testdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:34:01.762376Z",
     "iopub.status.busy": "2025-05-22T13:34:01.762114Z",
     "iopub.status.idle": "2025-05-22T13:34:02.714351Z",
     "shell.execute_reply": "2025-05-22T13:34:02.713586Z",
     "shell.execute_reply.started": "2025-05-22T13:34:01.762357Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Hindi word length: 19\n",
      "Total unique Hindi characters: 63\n",
      "['ँ', 'ं', 'ः', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ऑ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह', '़', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॅ', 'े', 'ै', 'ॉ', 'ो', 'ौ', '्', 'ॐ']\n"
     ]
    }
   ],
   "source": [
    "Hindi_vocab = set()\n",
    "max_hin_len = 0\n",
    "\n",
    "# Process training data\n",
    "for x in range(len(traindata)):\n",
    "    native_word = traindata.iloc[x]['native']\n",
    "    if pd.isna(native_word):\n",
    "        continue\n",
    "\n",
    "    max_hin_len = max(max_hin_len, len(native_word))\n",
    "    for char in native_word:\n",
    "        Hindi_vocab.add(char)\n",
    "\n",
    "# Process test data\n",
    "for x in range(len(testdata)):\n",
    "    native_word = testdata.iloc[x]['native']\n",
    "    if pd.isna(native_word):\n",
    "        continue\n",
    "\n",
    "    for char in native_word:\n",
    "        if char not in Hindi_vocab:\n",
    "            print(char)\n",
    "            Hindi_vocab.add(char)\n",
    "\n",
    "\n",
    "Hindi_vocab = sorted(Hindi_vocab)\n",
    "\n",
    "print(\"Max Hindi word length:\", max_hin_len)\n",
    "print(\"Total unique Hindi characters:\", len(Hindi_vocab))\n",
    "print(Hindi_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:34:05.241538Z",
     "iopub.status.busy": "2025-05-22T13:34:05.241269Z",
     "iopub.status.idle": "2025-05-22T13:34:06.070030Z",
     "shell.execute_reply": "2025-05-22T13:34:06.069164Z",
     "shell.execute_reply.started": "2025-05-22T13:34:05.241518Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max english word length: 20\n",
      "Total unique english characters: 26\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "max_eng_len = 0\n",
    "English_vocab = set() \n",
    "\n",
    "for x in range(len(traindata)):\n",
    "    latin_word = traindata.iloc[x]['latin']\n",
    "    \n",
    "    if pd.isna(latin_word):\n",
    "        continue  # skip if value is missing\n",
    "\n",
    "    word_len = len(latin_word)\n",
    "    max_eng_len = max(max_eng_len, word_len)\n",
    "\n",
    "    for char in latin_word:\n",
    "        English_vocab.add(char)\n",
    "\n",
    "# Convert set to sorted list\n",
    "English_vocab = sorted(English_vocab)\n",
    "print(\"Max english word length:\", max_eng_len)\n",
    "print(\"Total unique english characters:\", len(English_vocab))\n",
    "print(English_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Loader (Data Preprocessing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:34:11.164068Z",
     "iopub.status.busy": "2025-05-22T13:34:11.163800Z",
     "iopub.status.idle": "2025-05-22T13:34:11.179947Z",
     "shell.execute_reply": "2025-05-22T13:34:11.179223Z",
     "shell.execute_reply.started": "2025-05-22T13:34:11.164046Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.source_texts = []\n",
    "        self.target_texts = []\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
    "            \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                parts = line.strip().split('\\t')\n",
    "                \n",
    "                # Handle different file formats\n",
    "                if len(parts) == 3:  # Format: Hindi\\tRomanized\\tFrequency\n",
    "                    target, source, _ = parts\n",
    "                    self.source_texts.append(source)\n",
    "                    self.target_texts.append(target)\n",
    "                elif len(parts) == 2:  # Format: Hindi\\tRomanized\n",
    "                    target, source = parts\n",
    "                    self.source_texts.append(source)\n",
    "                    self.target_texts.append(target)\n",
    "                else:\n",
    "                    print(f\"Warning: Line {line_num} has unexpected format (expected 2 or 3 columns): {line.strip()}\")\n",
    "        \n",
    "        # If no data was loaded, raise an error\n",
    "        if len(self.source_texts) == 0:\n",
    "            raise ValueError(f\"No data was loaded from {file_path}. File may be empty or incorrectly formatted.\")\n",
    "                    \n",
    "        self.source_char_set = set()\n",
    "        self.target_char_set = set()\n",
    "        \n",
    "        for source in self.source_texts:\n",
    "            self.source_char_set.update(source)\n",
    "        \n",
    "        for target in self.target_texts:\n",
    "            self.target_char_set.update(target)\n",
    "        \n",
    "        self.source_char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted(self.source_char_set))}\n",
    "        self.source_idx_to_char = {idx + 1: char for idx, char in enumerate(sorted(self.source_char_set))}\n",
    "        self.source_char_to_idx['<PAD>'] = 0\n",
    "        self.source_idx_to_char[0] = '<PAD>'\n",
    "        self.source_char_to_idx['< SOS >'] = len(self.source_char_to_idx)\n",
    "        self.source_idx_to_char[len(self.source_idx_to_char)] = '< SOS >'\n",
    "        self.source_char_to_idx['<EOS>'] = len(self.source_char_to_idx)\n",
    "        self.source_idx_to_char[len(self.source_idx_to_char)] = '<EOS>'\n",
    "        \n",
    "        self.target_char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted(self.target_char_set))}\n",
    "        self.target_idx_to_char = {idx + 1: char for idx, char in enumerate(sorted(self.target_char_set))}\n",
    "        self.target_char_to_idx['<PAD>'] = 0\n",
    "        self.target_idx_to_char[0] = '<PAD>'\n",
    "        self.target_char_to_idx['< SOS >'] = len(self.target_char_to_idx)\n",
    "        self.target_idx_to_char[len(self.target_idx_to_char)] = '< SOS >'\n",
    "        self.target_char_to_idx['<EOS>'] = len(self.target_char_to_idx)\n",
    "        self.target_idx_to_char[len(self.target_idx_to_char)] = '<EOS>'\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.source_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = self.source_texts[idx]\n",
    "        target = self.target_texts[idx]\n",
    "        \n",
    "        source_indices = [self.source_char_to_idx['< SOS >']] + [self.source_char_to_idx[char] for char in source] + [self.source_char_to_idx['<EOS>']]\n",
    "        target_indices = [self.target_char_to_idx['< SOS >']] + [self.target_char_to_idx[char] for char in target] + [self.target_char_to_idx['<EOS>']]\n",
    "        \n",
    "        return {\n",
    "            'source': source,\n",
    "            'target': target,\n",
    "            'source_indices': source_indices,\n",
    "            'target_indices': target_indices,\n",
    "            'source_length': len(source_indices),\n",
    "            'target_length': len(target_indices)\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def source_vocab_size(self):\n",
    "        return len(self.source_char_to_idx)\n",
    "    \n",
    "    @property\n",
    "    def target_vocab_size(self):\n",
    "        return len(self.target_char_to_idx)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_source_len = max([len(item['source_indices']) for item in batch])\n",
    "    max_target_len = max([len(item['target_indices']) for item in batch])\n",
    "    \n",
    "    source_batch = []\n",
    "    target_batch = []\n",
    "    source_lengths = []\n",
    "    target_lengths = []\n",
    "    \n",
    "    source_texts = []\n",
    "    target_texts = []\n",
    "    \n",
    "    for item in batch:\n",
    "        source_padded = item['source_indices'] + [0] * (max_source_len - len(item['source_indices']))\n",
    "        target_padded = item['target_indices'] + [0] * (max_target_len - len(item['target_indices']))\n",
    "        \n",
    "        source_batch.append(source_padded)\n",
    "        target_batch.append(target_padded)\n",
    "        source_lengths.append(item['source_length'])\n",
    "        target_lengths.append(item['target_length'])\n",
    "        \n",
    "        source_texts.append(item['source'])\n",
    "        target_texts.append(item['target'])\n",
    "    \n",
    "    return {\n",
    "        'source': torch.LongTensor(source_batch),\n",
    "        'target': torch.LongTensor(target_batch),\n",
    "        'source_lengths': torch.LongTensor(source_lengths),\n",
    "        'target_lengths': torch.LongTensor(target_lengths),\n",
    "        'source_texts': source_texts,\n",
    "        'target_texts': target_texts\n",
    "    }\n",
    "\n",
    "def get_dataloader(file_path, batch_size, shuffle=True):\n",
    "    try:\n",
    "        dataset = TransliterationDataset(file_path)\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        return dataloader, dataset\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        print(f\"Please make sure the Dakshina dataset is downloaded and located correctly.\")\n",
    "        print(f\"Expected file path: {file_path}\")\n",
    "        print(f\"Current working directory: {os.getcwd()}\")\n",
    "        raise\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seq2Seq Model Vanila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:34:21.105910Z",
     "iopub.status.busy": "2025-05-22T13:34:21.105642Z",
     "iopub.status.idle": "2025-05-22T13:34:21.125386Z",
     "shell.execute_reply": "2025-05-22T13:34:21.124862Z",
     "shell.execute_reply.started": "2025-05-22T13:34:21.105892Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, num_layers, dropout, cell_type=\"GRU\"):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if cell_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(\n",
    "                embed_size,\n",
    "                hidden_size,\n",
    "                num_layers,\n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "            )\n",
    "        elif cell_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(\n",
    "                embed_size,\n",
    "                hidden_size,\n",
    "                num_layers,\n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "            )\n",
    "        else:  # Default to RNN\n",
    "            self.rnn = nn.RNN(\n",
    "                embed_size,\n",
    "                hidden_size,\n",
    "                num_layers,\n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "            )\n",
    "        \n",
    "        self.cell_type = cell_type\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        # Pack padded batch of sequences for RNN\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        outputs, hidden = self.rnn(packed)\n",
    "        \n",
    "        # Unpack padding\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        \n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embed_size, hidden_size, num_layers, dropout, cell_type=\"GRU\"):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cell_type = cell_type\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if cell_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(\n",
    "                embed_size,\n",
    "                hidden_size,\n",
    "                num_layers,\n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "            )\n",
    "        elif cell_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(\n",
    "                embed_size,\n",
    "                hidden_size,\n",
    "                num_layers,\n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "            )\n",
    "        else:  # Default to RNN\n",
    "            self.rnn = nn.RNN(\n",
    "                embed_size,\n",
    "                hidden_size,\n",
    "                num_layers,\n",
    "                dropout=dropout if num_layers > 1 else 0,\n",
    "                batch_first=True\n",
    "            )\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # x shape: (batch_size, 1)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        # Forward propagate through RNN\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        \n",
    "        # Compute output\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        \n",
    "        return prediction, hidden\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 output_size, \n",
    "                 embed_size, \n",
    "                 hidden_size, \n",
    "                 num_encoder_layers, \n",
    "                 num_decoder_layers, \n",
    "                 dropout, \n",
    "                 cell_type=\"GRU\"):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            input_size, \n",
    "            embed_size, \n",
    "            hidden_size, \n",
    "            num_encoder_layers, \n",
    "            dropout, \n",
    "            cell_type\n",
    "        )\n",
    "        \n",
    "        self.decoder = Decoder(\n",
    "            output_size, \n",
    "            embed_size, \n",
    "            hidden_size, \n",
    "            num_decoder_layers, \n",
    "            dropout, \n",
    "            cell_type\n",
    "        )\n",
    "        \n",
    "        self.cell_type = cell_type\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def forward(self, source, target, source_lengths, teacher_forcing_ratio=0.5):\n",
    "        batch_size = source.shape[0]\n",
    "        target_len = target.shape[1]\n",
    "        target_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(source.device)\n",
    "        \n",
    "        # Encode source sequences\n",
    "        encoder_outputs, hidden = self.encoder(source, source_lengths)\n",
    "        \n",
    "        # Process hidden state if encoder and decoder layers differ\n",
    "        hidden = self._process_hidden_for_decoder(hidden, batch_size)\n",
    "        \n",
    "        # First input to the decoder is the < SOS > token\n",
    "        decoder_input = target[:, 0].unsqueeze(1)\n",
    "        \n",
    "        # Decode one character at a time\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden = self.decoder(decoder_input, hidden)\n",
    "            outputs[:, t, :] = output\n",
    "            \n",
    "            # Teacher forcing: decide whether to use the predicted or actual target as next input\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # Get the highest predicted token\n",
    "            top1 = output.argmax(1)\n",
    "            \n",
    "            # If teacher forcing, use actual next token as input; otherwise use predicted token\n",
    "            decoder_input = target[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def _process_hidden_for_decoder(self, hidden, batch_size):\n",
    "        \"\"\"Process the encoder hidden state for the decoder\"\"\"\n",
    "        if self.num_encoder_layers == self.num_decoder_layers:\n",
    "            return hidden\n",
    "        \n",
    "        # For GRU and RNN\n",
    "        if self.cell_type != \"LSTM\":\n",
    "            if self.num_encoder_layers < self.num_decoder_layers:\n",
    "                # Duplicate last layer to match decoder layers\n",
    "                additional_layers = self.num_decoder_layers - self.num_encoder_layers\n",
    "                last_layer = hidden[-1:].expand(additional_layers, batch_size, self.hidden_size)\n",
    "                return torch.cat([hidden, last_layer], dim=0)\n",
    "            else:\n",
    "                # Use only the last n layers\n",
    "                return hidden[-self.num_decoder_layers:]\n",
    "        else:\n",
    "            # For LSTM (hidden is a tuple of (h_0, c_0))\n",
    "            h, c = hidden\n",
    "            if self.num_encoder_layers < self.num_decoder_layers:\n",
    "                # Duplicate last layer to match decoder layers\n",
    "                additional_layers = self.num_decoder_layers - self.num_encoder_layers\n",
    "                last_h_layer = h[-1:].expand(additional_layers, batch_size, self.hidden_size)\n",
    "                last_c_layer = c[-1:].expand(additional_layers, batch_size, self.hidden_size)\n",
    "                new_h = torch.cat([h, last_h_layer], dim=0)\n",
    "                new_c = torch.cat([c, last_c_layer], dim=0)\n",
    "                return (new_h, new_c)\n",
    "            else:\n",
    "                # Use only the last n layers\n",
    "                return (h[-self.num_decoder_layers:], c[-self.num_decoder_layers:])\n",
    "    \n",
    "    def predict(self, source, source_len, target_vocab_size, sos_idx, eos_idx, max_len=100):\n",
    "        batch_size = source.shape[0]\n",
    "        \n",
    "        # Encode source sequences\n",
    "        encoder_outputs, hidden = self.encoder(source, source_len)\n",
    "        \n",
    "        # Process hidden state if encoder and decoder layers differ\n",
    "        hidden = self._process_hidden_for_decoder(hidden, batch_size)\n",
    "        \n",
    "        # First input to the decoder is the < SOS > token\n",
    "        decoder_input = torch.tensor([[sos_idx]] * batch_size).to(source.device)\n",
    "        \n",
    "        # Lists to store predicted outputs\n",
    "        predictions = []\n",
    "        \n",
    "        # Flag to indicate if decoding is complete\n",
    "        done = [False] * batch_size\n",
    "        \n",
    "        # Decode one character at a time\n",
    "        for _ in range(max_len):\n",
    "            output, hidden = self.decoder(decoder_input, hidden)\n",
    "            \n",
    "            # Get the highest predicted token\n",
    "            top1 = output.argmax(1)\n",
    "            \n",
    "            # Store predicted token\n",
    "            predictions.append(top1.unsqueeze(1))\n",
    "            \n",
    "            # Check if all sequences have reached EOS\n",
    "            for i in range(batch_size):\n",
    "                if top1[i].item() == eos_idx:\n",
    "                    done[i] = True\n",
    "            \n",
    "            if all(done):\n",
    "                break\n",
    "            \n",
    "            # Use predicted token as next input\n",
    "            decoder_input = top1.unsqueeze(1)\n",
    "        \n",
    "        # Concatenate predictions along the sequence dimension\n",
    "        return torch.cat(predictions, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### utilities function used in training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:34:24.308432Z",
     "iopub.status.busy": "2025-05-22T13:34:24.308173Z",
     "iopub.status.idle": "2025-05-22T13:34:24.317465Z",
     "shell.execute_reply": "2025-05-22T13:34:24.316891Z",
     "shell.execute_reply.started": "2025-05-22T13:34:24.308413Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def calculate_accuracy(predictions, targets, target_lengths, ignore_index=0):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the predictions against the targets.\n",
    "    A prediction is considered correct only if all characters in the sequence match.\n",
    "    \"\"\"\n",
    "    batch_size = predictions.size(0)\n",
    "    correct = 0\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        pred_seq = predictions[i].cpu().numpy()\n",
    "        target_seq = targets[i, 1:target_lengths[i]-1].cpu().numpy()  # Exclude SOS and EOS tokens\n",
    "        \n",
    "        # Check if prediction exactly matches target\n",
    "        if len(pred_seq) == len(target_seq) and np.array_equal(pred_seq, target_seq):\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / batch_size\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, accuracy, filename):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'accuracy': accuracy\n",
    "    }, filename)\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename):\n",
    "    \"\"\"Load model checkpoint\"\"\"\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    accuracy = checkpoint.get('accuracy', 0.0)\n",
    "    return model, optimizer, epoch, accuracy\n",
    "\n",
    "def indices_to_string(indices, idx_to_char, eos_idx=None):\n",
    "    \"\"\"Convert a sequence of indices to a string\"\"\"\n",
    "    if eos_idx is not None:\n",
    "        # Find the index of the first EOS token\n",
    "        try:\n",
    "            eos_pos = indices.index(eos_idx)\n",
    "            indices = indices[:eos_pos]\n",
    "        except ValueError:\n",
    "            pass  # No EOS token found\n",
    "    \n",
    "    return ''.join([idx_to_char[idx] for idx in indices if idx in idx_to_char and idx_to_char[idx] not in ['<PAD>', '< SOS >', '<EOS>']])\n",
    "\n",
    "def create_directory(directory):\n",
    "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### config for training on a fixed parameter(model,training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:32:12.365484Z",
     "iopub.status.busy": "2025-05-20T12:32:12.365188Z",
     "iopub.status.idle": "2025-05-20T12:32:12.382642Z",
     "shell.execute_reply": "2025-05-20T12:32:12.382155Z",
     "shell.execute_reply.started": "2025-05-20T12:32:12.365463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Config:\n",
    "    # Data parameters\n",
    "    language = os.environ.get(\"TRANSLITERATION_LANGUAGE\", \"hi\")  # Get language from env var or default to \"hi\"\n",
    "    #base_dir = os.path.dirname(os.path.abspath(__file__))  # Get the directory of the current file\n",
    "    #data_dir = os.path.join(base_dir, \"dakshina_dataset_v1.0\", f\"{language}\", \"lexicons\") \n",
    "    input_dir = \"/kaggle/input/dataset\"  # Read-only input path\n",
    "    base_dir = \"/kaggle/working\"         # Writable path for outputs\n",
    "    \n",
    "    data_dir = os.path.join(input_dir, \"dakshina_dataset_v1.0\", language, \"lexicons\")\n",
    "    # Check if data directory exists, if not, try to find it elsewhere\n",
    "    if not os.path.exists(data_dir):\n",
    "        # Try to find the dakshina dataset in parent directories\n",
    "        parent_dir = os.path.dirname(base_dir)\n",
    "        alt_data_dir = os.path.join(parent_dir, \"dakshina_dataset_v1.0\", f\"{language}\", \"lexicons\")\n",
    "        if os.path.exists(alt_data_dir):\n",
    "            data_dir = alt_data_dir\n",
    "    \n",
    "    train_file = os.path.join(data_dir, f\"{language}.translit.sampled.train.tsv\")\n",
    "    val_file = os.path.join(data_dir, f\"{language}.translit.sampled.dev.tsv\")\n",
    "    test_file = os.path.join(data_dir, f\"{language}.translit.sampled.test.tsv\")\n",
    "    \n",
    "    # Model parameters (default values)\n",
    "    embed_size = 64\n",
    "    hidden_size = 128\n",
    "    num_encoder_layers = 1\n",
    "    num_decoder_layers = 1\n",
    "    dropout = 0.2\n",
    "    cell_type = \"GRU\"  # Options: RNN, LSTM, GRU\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size = 64\n",
    "    epochs = 20\n",
    "    learning_rate = 0.001\n",
    "    teacher_forcing_ratio = 0.5\n",
    "    \n",
    "    # Decoding parameters\n",
    "    beam_size = 1  # 1 for greedy decoding\n",
    "    \n",
    "    # W&B parameters\n",
    "    wandb_project = \"DL_A3_seq2seq\"\n",
    "    wandb_entity = \"da24m006-iit-madras\"  \n",
    "    \n",
    "    # Save directories\n",
    "    model_dir = os.path.join(base_dir, \"saved_models\", language)\n",
    "    # Create directory for saving model if it doesn't exist\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    prediction_dir = os.path.join(base_dir, \"predictions\", language)\n",
    "    # Create directory for saving prediction if it doesn't exist\n",
    "    if not os.path.exists(prediction_dir):\n",
    "        os.makedirs(prediction_dir, exist_ok=True)\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Verify paths and print info\n",
    "        print(f\"Base directory: {self.base_dir}\")\n",
    "        print(f\"Language: {self.language}\")\n",
    "        print(f\"Data directory: {self.data_dir}\")\n",
    "        print(f\"Train file exists: {os.path.exists(self.train_file)}\")\n",
    "        print(f\"Val file exists: {os.path.exists(self.val_file)}\")\n",
    "        print(f\"Test file exists: {os.path.exists(self.test_file)}\")\n",
    "        \n",
    "        # Print sample data from files if they exist\n",
    "        self._print_sample_data(self.train_file, \"train\")\n",
    "        self._print_sample_data(self.val_file, \"validation\")\n",
    "        self._print_sample_data(self.test_file, \"test\")\n",
    "    \n",
    "    def _print_sample_data(self, file_path, name):\n",
    "        \"\"\"Print a sample of data from the file for debugging\"\"\"\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    lines = f.readlines()[:3]  # Get first 3 lines\n",
    "                    if lines:\n",
    "                        print(f\"\\nSample {name} data:\")\n",
    "                        for line in lines:\n",
    "                            print(f\"  {line.strip()}\")\n",
    "                        print(f\"  ... (total lines: {sum(1 for _ in open(file_path, 'r', encoding='utf-8'))})\")\n",
    "                    else:\n",
    "                        print(f\"\\nWarning: {name} file exists but is empty: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError reading {name} file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training with a fixed config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:32:12.383657Z",
     "iopub.status.busy": "2025-05-20T12:32:12.383428Z",
     "iopub.status.idle": "2025-05-20T12:32:12.404555Z",
     "shell.execute_reply": "2025-05-20T12:32:12.403859Z",
     "shell.execute_reply.started": "2025-05-20T12:32:12.383637Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from config import Config\n",
    "# from dataloader import get_dataloader\n",
    "# from seq2seq import Seq2Seq\n",
    "# from utils import save_checkpoint, create_directory, indices_to_string\n",
    "\n",
    "def train(config, run_name=None, run_already_initialized=False):\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize W&B only if it's not already initialized\n",
    "    if not run_already_initialized:\n",
    "        if run_name:\n",
    "            wandb.init(project=config.wandb_project, entity=config.wandb_entity, name=run_name, config=vars(config))\n",
    "        else:\n",
    "            # Include language in the run name if not provided\n",
    "            default_run_name = f\"{config.cell_type}_lang_{config.language}_e{config.embed_size}_h{config.hidden_size}\"\n",
    "            wandb.init(project=config.wandb_project, entity=config.wandb_entity, name=default_run_name, config=vars(config))\n",
    "    \n",
    "        # Explicitly log the language being used\n",
    "        wandb.config.update({\"language\": config.language})\n",
    "    \n",
    "    print(f\"\\nTraining transliteration model for language: {config.language}\")\n",
    "    print(f\"Using dataset from: {config.data_dir}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        train_loader, train_dataset = get_dataloader(\n",
    "            config.train_file, \n",
    "            config.batch_size, \n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_loader, val_dataset = get_dataloader(\n",
    "            config.val_file, \n",
    "            config.batch_size, \n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        model = Seq2Seq(\n",
    "            input_size=train_dataset.source_vocab_size,\n",
    "            output_size=train_dataset.target_vocab_size,\n",
    "            embed_size=config.embed_size,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_encoder_layers=config.num_encoder_layers,\n",
    "            num_decoder_layers=config.num_decoder_layers,\n",
    "            dropout=config.dropout,\n",
    "            cell_type=config.cell_type\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "        \n",
    "        # Create directory for saving models\n",
    "        create_directory(config.model_dir)\n",
    "        \n",
    "        best_accuracy = 0.0\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(config.epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config.epochs}')\n",
    "            \n",
    "            for batch_idx, batch in enumerate(pbar):\n",
    "                # Move tensors to device\n",
    "                source = batch['source'].to(device)\n",
    "                target = batch['target'].to(device)\n",
    "                source_lengths = batch['source_lengths']\n",
    "                \n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                output = model(source, target, source_lengths, config.teacher_forcing_ratio)\n",
    "                \n",
    "                # Calculate loss\n",
    "                # Reshape output and target for loss calculation\n",
    "                output_dim = output.shape[-1]\n",
    "                output = output[:, 1:].reshape(-1, output_dim)  # Remove SOS token\n",
    "                target = target[:, 1:].reshape(-1)  # Remove SOS token\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Clip gradients\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "            # Calculate average loss\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            model.eval()\n",
    "            val_loss, val_accuracy = evaluate(model, val_loader, criterion, device, train_dataset)\n",
    "            \n",
    "            # Log metrics to W&B\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': avg_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'language': config.language  # Log language with metrics\n",
    "            })\n",
    "            \n",
    "            print(f'Epoch: {epoch+1}, Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "            \n",
    "            # Save model if it's the best so far\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "                if not os.path.exists(os.path.join(config.model_dir)):\n",
    "                    os.makedirs(os.path.join(config.model_dir))\n",
    "\n",
    "                # Save locally first with language code in filename\n",
    "                model_path = os.path.join(config.model_dir, f'best_model_{config.language}_{wandb.run.id}.pt')\n",
    "                save_checkpoint(\n",
    "                    model, \n",
    "                    optimizer, \n",
    "                    epoch, \n",
    "                    val_accuracy, \n",
    "                    model_path\n",
    "                )\n",
    "                \n",
    "                # Save model state dictionary to W&B\n",
    "                try:\n",
    "                    # Save model for W&B\n",
    "                    wandb_model_path = os.path.join(wandb.run.dir, \"best_model.pt\")\n",
    "                    torch.save(model.state_dict(), wandb_model_path)\n",
    "                    \n",
    "                    # Use W&B Artifact API instead of direct file save\n",
    "                    artifact = wandb.Artifact(f\"model-{config.language}-{wandb.run.id}\", type=\"model\")\n",
    "                    artifact.add_file(wandb_model_path)\n",
    "                    wandb.log_artifact(artifact)\n",
    "                    \n",
    "                    print(f\"Model successfully saved to W&B as artifact\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to save model to W&B: {str(e)}\")\n",
    "                    print(f\"Model was saved locally to {model_path}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                checkpoint_path = os.path.join(config.model_dir, f'checkpoint_{config.language}_{epoch+1}_{wandb.run.id}.pt')\n",
    "                save_checkpoint(\n",
    "                    model, \n",
    "                    optimizer, \n",
    "                    epoch, \n",
    "                    val_accuracy, \n",
    "                    checkpoint_path\n",
    "                )\n",
    "\n",
    "        # Only finish wandb if we started it in this function\n",
    "        if not run_already_initialized:\n",
    "            wandb.finish()\n",
    "        \n",
    "        return model, best_accuracy\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {str(e)}\")\n",
    "        # Only finish wandb if we started it in this function\n",
    "        if not run_already_initialized:\n",
    "            wandb.finish()\n",
    "        # Re-raise the exception for proper error handling\n",
    "        raise\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, dataset):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move tensors to device\n",
    "            source = batch['source'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "            source_lengths = batch['source_lengths']\n",
    "            target_lengths = batch['target_lengths']\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(source, target, source_lengths, teacher_forcing_ratio=0)\n",
    "            \n",
    "            # Calculate loss\n",
    "            output_dim = output.shape[-1]\n",
    "            output_for_loss = output[:, 1:].reshape(-1, output_dim)  # Remove SOS token\n",
    "            target_for_loss = target[:, 1:].reshape(-1)  # Remove SOS token\n",
    "            \n",
    "            loss = criterion(output_for_loss, target_for_loss)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Generate predictions\n",
    "            predictions = model.predict(\n",
    "                source,\n",
    "                source_lengths,\n",
    "                dataset.target_vocab_size,\n",
    "                dataset.target_char_to_idx['< SOS >'],\n",
    "                dataset.target_char_to_idx['<EOS>']\n",
    "            )\n",
    "            \n",
    "            # Calculate accuracy (exact match)\n",
    "            for i in range(len(predictions)):\n",
    "                pred_seq = [idx.item() for idx in predictions[i]]\n",
    "                pred_str = indices_to_string(pred_seq, dataset.target_idx_to_char, dataset.target_char_to_idx['<EOS>'])\n",
    "                target_str = batch['target_texts'][i]\n",
    "                \n",
    "                if pred_str == target_str:\n",
    "                    total_correct += 1\n",
    "            \n",
    "            total_examples += len(predictions)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_examples\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     config = Config()\n",
    "#     train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T12:32:12.405398Z",
     "iopub.status.busy": "2025-05-20T12:32:12.405211Z",
     "iopub.status.idle": "2025-05-20T12:40:58.462374Z",
     "shell.execute_reply": "2025-05-20T12:40:58.461787Z",
     "shell.execute_reply.started": "2025-05-20T12:32:12.405383Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: /kaggle/working\n",
      "Language: hi\n",
      "Data directory: /kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons\n",
      "Train file exists: True\n",
      "Val file exists: True\n",
      "Test file exists: True\n",
      "\n",
      "Sample train data:\n",
      "  अं\tan\t3\n",
      "  अंकगणित\tankganit\t3\n",
      "  अंकल\tuncle\t4\n",
      "  ... (total lines: 44204)\n",
      "\n",
      "Sample validation data:\n",
      "  अंकन\tankan\t3\n",
      "  अंगकोर\tangkor\t3\n",
      "  अंगिरा\tangira\t3\n",
      "  ... (total lines: 4358)\n",
      "\n",
      "Sample test data:\n",
      "  अंक\tank\t5\n",
      "  अंक\tanka\t1\n",
      "  अंकित\tankit\t3\n",
      "  ... (total lines: 4502)\n",
      "Create sweep with ID: 2nykpam1\n",
      "Sweep URL: https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/2nykpam1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 21hw6j60 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: /kaggle/working\n",
      "Language: hi\n",
      "Data directory: /kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons\n",
      "Train file exists: True\n",
      "Val file exists: True\n",
      "Test file exists: True\n",
      "\n",
      "Sample train data:\n",
      "  अं\tan\t3\n",
      "  अंकगणित\tankganit\t3\n",
      "  अंकल\tuncle\t4\n",
      "  ... (total lines: 44204)\n",
      "\n",
      "Sample validation data:\n",
      "  अंकन\tankan\t3\n",
      "  अंगकोर\tangkor\t3\n",
      "  अंगिरा\tangira\t3\n",
      "  ... (total lines: 4358)\n",
      "\n",
      "Sample test data:\n",
      "  अंक\tank\t5\n",
      "  अंक\tanka\t1\n",
      "  अंकित\tankit\t3\n",
      "  ... (total lines: 4502)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_123220-21hw6j60</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/21hw6j60' target=\"_blank\">GRU_lang_hi_drop0.2_es64_hs128_en1_de1_lr0.001</a></strong> to <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/2nykpam1' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/2nykpam1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/2nykpam1' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/2nykpam1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/21hw6j60' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/21hw6j60</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training transliteration model for language: hi\n",
      "Using dataset from: /kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 346/346 [00:11<00:00, 31.41it/s, loss=2.69]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 2.9249, Val Loss: 5.9429, Val Accuracy: 0.0002\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 346/346 [00:09<00:00, 34.68it/s, loss=2.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 2.5464, Val Loss: 6.2105, Val Accuracy: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 346/346 [00:09<00:00, 34.74it/s, loss=2.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 2.3649, Val Loss: 6.5484, Val Accuracy: 0.0005\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 346/346 [00:09<00:00, 34.75it/s, loss=2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 2.3168, Val Loss: 6.5979, Val Accuracy: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 346/346 [00:09<00:00, 34.70it/s, loss=2.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train Loss: 2.2576, Val Loss: 6.5455, Val Accuracy: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 346/346 [00:10<00:00, 34.33it/s, loss=2.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train Loss: 2.2027, Val Loss: 6.5478, Val Accuracy: 0.0009\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 346/346 [00:09<00:00, 34.84it/s, loss=2.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train Loss: 2.1637, Val Loss: 6.7259, Val Accuracy: 0.0021\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 346/346 [00:09<00:00, 34.86it/s, loss=2.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train Loss: 2.1121, Val Loss: 6.8244, Val Accuracy: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 346/346 [00:10<00:00, 34.50it/s, loss=2.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train Loss: 2.0687, Val Loss: 6.7650, Val Accuracy: 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 346/346 [00:09<00:00, 34.75it/s, loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train Loss: 2.0406, Val Loss: 6.9397, Val Accuracy: 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 346/346 [00:10<00:00, 34.27it/s, loss=1.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train Loss: 2.0232, Val Loss: 7.0162, Val Accuracy: 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 346/346 [00:09<00:00, 34.68it/s, loss=2.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train Loss: 1.9893, Val Loss: 7.1441, Val Accuracy: 0.0048\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 346/346 [00:09<00:00, 34.64it/s, loss=2.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train Loss: 1.9623, Val Loss: 7.1340, Val Accuracy: 0.0057\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 346/346 [00:10<00:00, 34.17it/s, loss=1.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train Loss: 1.9285, Val Loss: 7.2103, Val Accuracy: 0.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 346/346 [00:09<00:00, 34.83it/s, loss=1.69]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train Loss: 1.8842, Val Loss: 7.4220, Val Accuracy: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 346/346 [00:09<00:00, 34.86it/s, loss=2.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train Loss: 1.8853, Val Loss: 7.5029, Val Accuracy: 0.0085\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 346/346 [00:09<00:00, 34.73it/s, loss=2.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train Loss: 1.8586, Val Loss: 7.5774, Val Accuracy: 0.0080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 346/346 [00:09<00:00, 34.77it/s, loss=1.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train Loss: 1.8114, Val Loss: 7.6548, Val Accuracy: 0.0096\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 346/346 [00:10<00:00, 34.43it/s, loss=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train Loss: 1.8078, Val Loss: 7.6194, Val Accuracy: 0.0103\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 346/346 [00:09<00:00, 34.64it/s, loss=1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train Loss: 1.7864, Val Loss: 7.6867, Val Accuracy: 0.0115\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▂▁▂▂▂▄▄▄▄▆▆▇▇█</td></tr><tr><td>val_loss</td><td>▁▂▃▄▃▃▄▅▄▅▅▆▆▆▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_accuracy</td><td>0.01147</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>language</td><td>hi</td></tr><tr><td>train_loss</td><td>1.78644</td></tr><tr><td>val_accuracy</td><td>0.01147</td></tr><tr><td>val_loss</td><td>7.68668</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">RNN_lang_hi_drop0.1_es256_hs256_en1_de3_lr0.001</strong> at: <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/21hw6j60' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/21hw6j60</a><br> View project at: <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq</a><br>Synced 5 W&B file(s), 0 media file(s), 20 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_123220-21hw6j60/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: aywcsa4z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decoder_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encoder_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: /kaggle/working\n",
      "Language: hi\n",
      "Data directory: /kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons\n",
      "Train file exists: True\n",
      "Val file exists: True\n",
      "Test file exists: True\n",
      "\n",
      "Sample train data:\n",
      "  अं\tan\t3\n",
      "  अंकगणित\tankganit\t3\n",
      "  अंकल\tuncle\t4\n",
      "  ... (total lines: 44204)\n",
      "\n",
      "Sample validation data:\n",
      "  अंकन\tankan\t3\n",
      "  अंगकोर\tangkor\t3\n",
      "  अंगिरा\tangira\t3\n",
      "  ... (total lines: 4358)\n",
      "\n",
      "Sample test data:\n",
      "  अंक\tank\t5\n",
      "  अंक\tanka\t1\n",
      "  अंकित\tankit\t3\n",
      "  ... (total lines: 4502)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_123644-aywcsa4z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/aywcsa4z' target=\"_blank\">GRU_lang_hi_drop0.2_es64_hs128_en1_de1_lr0.001</a></strong> to <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/2nykpam1' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/2nykpam1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/2nykpam1' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/2nykpam1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/aywcsa4z' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/aywcsa4z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training transliteration model for language: hi\n",
      "Using dataset from: /kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 346/346 [00:10<00:00, 33.67it/s, loss=2.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 3.2617, Val Loss: 5.1574, Val Accuracy: 0.0002\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 346/346 [00:10<00:00, 33.79it/s, loss=2.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 2.7966, Val Loss: 5.3025, Val Accuracy: 0.0007\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 346/346 [00:10<00:00, 33.27it/s, loss=2.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 2.4797, Val Loss: 5.3640, Val Accuracy: 0.0032\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 346/346 [00:10<00:00, 33.65it/s, loss=2.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 2.2161, Val Loss: 5.5354, Val Accuracy: 0.0069\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 346/346 [00:10<00:00, 32.73it/s, loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train Loss: 1.9969, Val Loss: 5.6624, Val Accuracy: 0.0236\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 346/346 [00:10<00:00, 33.56it/s, loss=1.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train Loss: 1.8182, Val Loss: 5.9109, Val Accuracy: 0.0402\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 346/346 [00:10<00:00, 33.46it/s, loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train Loss: 1.6749, Val Loss: 6.1019, Val Accuracy: 0.0622\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 346/346 [00:10<00:00, 32.84it/s, loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train Loss: 1.5604, Val Loss: 6.3468, Val Accuracy: 0.0771\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 346/346 [00:10<00:00, 33.86it/s, loss=1.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train Loss: 1.4699, Val Loss: 6.5012, Val Accuracy: 0.0895\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 346/346 [00:10<00:00, 33.36it/s, loss=1.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train Loss: 1.3909, Val Loss: 6.7180, Val Accuracy: 0.1072\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 346/346 [00:10<00:00, 33.62it/s, loss=1.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train Loss: 1.3200, Val Loss: 6.8955, Val Accuracy: 0.1223\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 346/346 [00:10<00:00, 33.43it/s, loss=1.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train Loss: 1.2677, Val Loss: 7.0548, Val Accuracy: 0.1374\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 346/346 [00:10<00:00, 32.92it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train Loss: 1.2194, Val Loss: 7.1799, Val Accuracy: 0.1528\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 346/346 [00:10<00:00, 33.62it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train Loss: 1.1751, Val Loss: 7.3024, Val Accuracy: 0.1652\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 346/346 [00:10<00:00, 33.56it/s, loss=1.07] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train Loss: 1.1321, Val Loss: 7.4378, Val Accuracy: 0.1801\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 346/346 [00:10<00:00, 33.54it/s, loss=1.09] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train Loss: 1.0956, Val Loss: 7.5276, Val Accuracy: 0.1916\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 346/346 [00:10<00:00, 34.29it/s, loss=1.08] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train Loss: 1.0617, Val Loss: 7.6597, Val Accuracy: 0.2047\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 346/346 [00:10<00:00, 33.30it/s, loss=0.807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train Loss: 1.0230, Val Loss: 7.7522, Val Accuracy: 0.2164\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 346/346 [00:10<00:00, 33.68it/s, loss=0.873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train Loss: 1.0084, Val Loss: 7.8390, Val Accuracy: 0.2198\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 346/346 [00:10<00:00, 32.12it/s, loss=0.974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train Loss: 0.9816, Val Loss: 7.8761, Val Accuracy: 0.2299\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▇▆▅▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▂▂▃▃▄▄▅▅▆▆▆▇▇███</td></tr><tr><td>val_loss</td><td>▁▁▂▂▂▃▃▄▄▅▅▆▆▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_accuracy</td><td>0.22992</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>language</td><td>hi</td></tr><tr><td>train_loss</td><td>0.98164</td></tr><tr><td>val_accuracy</td><td>0.22992</td></tr><tr><td>val_loss</td><td>7.87605</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GRU_lang_hi_drop0.2_es32_hs256_en2_de2_lr0.0001</strong> at: <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/aywcsa4z' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/aywcsa4z</a><br> View project at: <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq</a><br>Synced 5 W&B file(s), 0 media file(s), 40 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_123644-aywcsa4z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### import wandb\n",
    "import os\n",
    "import argparse\n",
    "# from config import Config\n",
    "# from train import train\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',  # or 'grid' or 'random'\n",
    "    'metric': {\n",
    "        'name': 'val_accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'embed_size': {\n",
    "            'values': [16, 32, 64, 128, 256]\n",
    "        },\n",
    "        'hidden_size': {\n",
    "            'values': [32, 64, 128, 256, 512]\n",
    "        },\n",
    "        'num_encoder_layers': {\n",
    "            'values': [1, 2, 3]\n",
    "        },\n",
    "        'num_decoder_layers': {\n",
    "            'values': [1, 2, 3]\n",
    "        },\n",
    "        'cell_type': {\n",
    "            'values': ['RNN', 'GRU', 'LSTM']\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0.1, 0.2, 0.3, 0.5]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [0.0001, 0.0005, 0.001]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [32, 64, 128]\n",
    "        },\n",
    "        'teacher_forcing_ratio': {\n",
    "            'values': [0.3, 0.5, 0.7]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "def sweep_train(config_defaults=None):\n",
    "    config = Config()  # initialize config early to construct run_name\n",
    "\n",
    "    # Construct run name before initializing wandb\n",
    "    run_name = f\"{config.cell_type}_{config.language}_drop{config.dropout}_es{config.embed_size}_hs{config.hidden_size}_en{config.num_encoder_layers}_de{config.num_decoder_layers}_lr{config.learning_rate}\"\n",
    "    \n",
    "    with wandb.init(config=config_defaults, name=run_name) as run:\n",
    "        wandb_config = wandb.config\n",
    "        for key, value in wandb_config.items():\n",
    "            if hasattr(config, key):\n",
    "                setattr(config, key, value)\n",
    "        \n",
    "        # Update run_name again after sweep parameters are applied\n",
    "        run_name = f\"{config.cell_type}_{config.language}_drop{config.dropout}_es{config.embed_size}_hs{config.hidden_size}_en{config.num_encoder_layers}_de{config.num_decoder_layers}_lr{config.learning_rate}\"\n",
    "        wandb.run.name = run_name\n",
    "        wandb.run.save()\n",
    "\n",
    "        # Train the model\n",
    "        model, accuracy = train(config, run_already_initialized=True)\n",
    "\n",
    "        # Log final result\n",
    "        run.summary['best_accuracy'] = accuracy\n",
    "\n",
    "# def sweep_train(config_defaults=None):\n",
    "#     # Initialize a new wandb run\n",
    "#     with wandb.init(config=config_defaults) as run:\n",
    "#         # Copy hyperparameters from wandb config\n",
    "#         config = Config()\n",
    "\n",
    "\n",
    "#         # Create model name based on parameters\n",
    "#         run_name = f\"{config.cell_type}_lang_{config.language}_drop{config.dropout}_es{config.embed_size}_hs{config.hidden_size}_en{config.num_encoder_layers}_de{config.num_decoder_layers}_lr{config.learning_rate}\"\n",
    "                \n",
    "#         # Update config with parameters from sweep\n",
    "#         wandb_config = wandb.config\n",
    "#         for key, value in wandb_config.items():\n",
    "#             if hasattr(config, key):\n",
    "#                 setattr(config, key, value)\n",
    "        \n",
    "        \n",
    "#         # Train the model\n",
    "#         # Note: We don't pass run_name here as wandb.init() is already handled in the train function\n",
    "#         # and we're using the current run within this context\n",
    "#         model, accuracy = train(config, run_already_initialized=True)\n",
    "        \n",
    "#         # Log the final result to the current run\n",
    "#         run.summary['best_accuracy'] = accuracy\n",
    "\n",
    "# def main():\n",
    "#     parser = argparse.ArgumentParser(description='Run W&B sweep for transliteration model')\n",
    "#     parser.add_argument('--sweep_id', type=str, help='The sweep ID to use (if already created)')\n",
    "#     parser.add_argument('--count', type=int, default=40, help='Number of sweep runs')\n",
    "#     parser.add_argument('--language', type=str, default='hi', \n",
    "#                         choices=['bn', 'gu', 'hi', 'kn', 'ml', 'mr', 'pa', 'sd', 'si', 'ta', 'te', 'ur'],\n",
    "#                         help='Language code to use (default: hi)')\n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     # Set language in environment variable so config.py can access it\n",
    "#     os.environ[\"TRANSLITERATION_LANGUAGE\"] = args.language\n",
    "    \n",
    "#     # Set up W&B project\n",
    "#     config = Config()\n",
    "    \n",
    "#     if args.sweep_id:\n",
    "#         sweep_id = args.sweep_id\n",
    "#     else:\n",
    "#         sweep_id = wandb.sweep(sweep=sweep_configuration, project=config.wandb_project, entity=config.wandb_entity)\n",
    "    \n",
    "#     # Start sweep agent\n",
    "#     wandb.agent(sweep_id, function=sweep_train, count=args.count)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "# os.environ[\"TRANSLITERATION_LANGUAGE\"] = \"hi\"\n",
    "# config = Config()\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=config.wandb_project, entity=config.wandb_entity)\n",
    "# wandb.agent(sweep_id, function=sweep_train, count=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Test uisng  Best Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:34:54.683449Z",
     "iopub.status.busy": "2025-05-22T13:34:54.682949Z",
     "iopub.status.idle": "2025-05-22T13:34:54.695115Z",
     "shell.execute_reply": "2025-05-22T13:34:54.694434Z",
     "shell.execute_reply.started": "2025-05-22T13:34:54.683426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Config:\n",
    "    # Data parameters\n",
    "    language = os.environ.get(\"TRANSLITERATION_LANGUAGE\", \"hi\")  # Get language from env var or default to \"hi\"\n",
    "    #base_dir = os.path.dirname(os.path.abspath(__file__))  # Get the directory of the current file\n",
    "    #data_dir = os.path.join(base_dir, \"dakshina_dataset_v1.0\", f\"{language}\", \"lexicons\") \n",
    "    input_dir = \"/kaggle/input/dataset\"  # Read-only input path\n",
    "    base_dir = \"/kaggle/working\"         # Writable path for outputs\n",
    "    \n",
    "    data_dir = os.path.join(input_dir, \"dakshina_dataset_v1.0\", language, \"lexicons\")\n",
    "    # Check if data directory exists, if not, try to find it elsewhere\n",
    "    if not os.path.exists(data_dir):\n",
    "        # Try to find the dakshina dataset in parent directories\n",
    "        parent_dir = os.path.dirname(base_dir)\n",
    "        alt_data_dir = os.path.join(parent_dir, \"dakshina_dataset_v1.0\", f\"{language}\", \"lexicons\")\n",
    "        if os.path.exists(alt_data_dir):\n",
    "            data_dir = alt_data_dir\n",
    "    \n",
    "    train_file = os.path.join(data_dir, f\"{language}.translit.sampled.train.tsv\")\n",
    "    val_file = os.path.join(data_dir, f\"{language}.translit.sampled.dev.tsv\")\n",
    "    test_file = os.path.join(data_dir, f\"{language}.translit.sampled.test.tsv\")\n",
    "    \n",
    "    # Model parameters \n",
    "    #Best model parameter\n",
    "    embed_size = 32\n",
    "    hidden_size = 512\n",
    "    num_encoder_layers = 3\n",
    "    num_decoder_layers = 3\n",
    "    dropout = 0.5\n",
    "    cell_type = \"LSTM\"  # Options: RNN, LSTM, GRU\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size = 64\n",
    "    epochs = 30\n",
    "    learning_rate = 0.001\n",
    "    teacher_forcing_ratio = 0.7\n",
    "    \n",
    "    # Decoding parameters\n",
    "    beam_size = 1  # 1 for greedy decoding\n",
    "    \n",
    "    # W&B parameters\n",
    "    wandb_project = \"DL_A3_seq2seq\"\n",
    "    wandb_entity = \"da24m006-iit-madras\"  \n",
    "    \n",
    "    # Save directories\n",
    "    model_dir = os.path.join(base_dir, \"saved_models\", language)\n",
    "    # Create directory for saving model if it doesn't exist\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    prediction_dir = os.path.join(base_dir, \"predictions\", language)\n",
    "    # Create directory for saving prediction if it doesn't exist\n",
    "    if not os.path.exists(prediction_dir):\n",
    "        os.makedirs(prediction_dir, exist_ok=True)\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Verify paths and print info\n",
    "        print(f\"Base directory: {self.base_dir}\")\n",
    "        print(f\"Language: {self.language}\")\n",
    "        print(f\"Data directory: {self.data_dir}\")\n",
    "        print(f\"Train file exists: {os.path.exists(self.train_file)}\")\n",
    "        print(f\"Val file exists: {os.path.exists(self.val_file)}\")\n",
    "        print(f\"Test file exists: {os.path.exists(self.test_file)}\")\n",
    "        \n",
    "        # Print sample data from files if they exist\n",
    "        self._print_sample_data(self.train_file, \"train\")\n",
    "        self._print_sample_data(self.val_file, \"validation\")\n",
    "        self._print_sample_data(self.test_file, \"test\")\n",
    "    \n",
    "    def _print_sample_data(self, file_path, name):\n",
    "        \"\"\"Print a sample of data from the file for debugging\"\"\"\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    lines = f.readlines()[:3]  # Get first 3 lines\n",
    "                    if lines:\n",
    "                        print(f\"\\nSample {name} data:\")\n",
    "                        for line in lines:\n",
    "                            print(f\"  {line.strip()}\")\n",
    "                        print(f\"  ... (total lines: {sum(1 for _ in open(file_path, 'r', encoding='utf-8'))})\")\n",
    "                    else:\n",
    "                        print(f\"\\nWarning: {name} file exists but is empty: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError reading {name} file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T12:14:43.278820Z",
     "iopub.status.busy": "2025-05-22T12:14:43.278183Z",
     "iopub.status.idle": "2025-05-22T12:29:57.419961Z",
     "shell.execute_reply": "2025-05-22T12:29:57.419207Z",
     "shell.execute_reply.started": "2025-05-22T12:14:43.278799Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: /kaggle/working\n",
      "Language: hi\n",
      "Data directory: /kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons\n",
      "Train file exists: True\n",
      "Val file exists: True\n",
      "Test file exists: True\n",
      "\n",
      "Sample train data:\n",
      "  अं\tan\t3\n",
      "  अंकगणित\tankganit\t3\n",
      "  अंकल\tuncle\t4\n",
      "  ... (total lines: 44204)\n",
      "\n",
      "Sample validation data:\n",
      "  अंकन\tankan\t3\n",
      "  अंगकोर\tangkor\t3\n",
      "  अंगिरा\tangira\t3\n",
      "  ... (total lines: 4358)\n",
      "\n",
      "Sample test data:\n",
      "  अंक\tank\t5\n",
      "  अंक\tanka\t1\n",
      "  अंकित\tankit\t3\n",
      "  ... (total lines: 4502)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250522_121443-ugfbcbw4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/ugfbcbw4' target=\"_blank\">LSTM_lang_hi_e32_h512</a></strong> to <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/ugfbcbw4' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/ugfbcbw4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training transliteration model for language: hi\n",
      "Using dataset from: /kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 691/691 [00:28<00:00, 24.63it/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 2.2526, Val Loss: 7.6023, Val Accuracy: 0.1212\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 691/691 [00:27<00:00, 25.36it/s, loss=0.715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 1.0375, Val Loss: 8.8094, Val Accuracy: 0.2845\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 691/691 [00:27<00:00, 25.26it/s, loss=0.592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 0.7326, Val Loss: 9.4153, Val Accuracy: 0.3469\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████| 691/691 [00:27<00:00, 25.24it/s, loss=0.466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 0.5922, Val Loss: 9.9825, Val Accuracy: 0.3731\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████| 691/691 [00:27<00:00, 25.15it/s, loss=0.568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train Loss: 0.5045, Val Loss: 10.4149, Val Accuracy: 0.3736\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████| 691/691 [00:27<00:00, 25.28it/s, loss=0.336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train Loss: 0.4353, Val Loss: 10.8946, Val Accuracy: 0.3846\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████| 691/691 [00:27<00:00, 25.11it/s, loss=0.448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train Loss: 0.3919, Val Loss: 10.9946, Val Accuracy: 0.4025\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████| 691/691 [00:27<00:00, 25.21it/s, loss=0.393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train Loss: 0.3518, Val Loss: 11.4813, Val Accuracy: 0.4087\n",
      "Model successfully saved to W&B as artifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████| 691/691 [00:27<00:00, 25.12it/s, loss=0.249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train Loss: 0.3178, Val Loss: 11.5065, Val Accuracy: 0.3928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|██████████| 691/691 [00:27<00:00, 25.36it/s, loss=0.281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train Loss: 0.2889, Val Loss: 11.9441, Val Accuracy: 0.4039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|██████████| 691/691 [00:27<00:00, 25.29it/s, loss=0.315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train Loss: 0.2666, Val Loss: 12.1308, Val Accuracy: 0.4048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|██████████| 691/691 [00:27<00:00, 25.28it/s, loss=0.385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train Loss: 0.2473, Val Loss: 12.6251, Val Accuracy: 0.3940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|██████████| 691/691 [00:27<00:00, 25.44it/s, loss=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train Loss: 0.2300, Val Loss: 12.8575, Val Accuracy: 0.3949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|██████████| 691/691 [00:27<00:00, 25.33it/s, loss=0.163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train Loss: 0.2212, Val Loss: 12.9552, Val Accuracy: 0.4011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|██████████| 691/691 [00:27<00:00, 25.20it/s, loss=0.189] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train Loss: 0.2075, Val Loss: 13.2469, Val Accuracy: 0.3910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|██████████| 691/691 [00:27<00:00, 25.22it/s, loss=0.115] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train Loss: 0.1945, Val Loss: 13.6430, Val Accuracy: 0.3871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|██████████| 691/691 [00:27<00:00, 25.19it/s, loss=0.166] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train Loss: 0.1823, Val Loss: 13.5998, Val Accuracy: 0.3889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|██████████| 691/691 [00:27<00:00, 25.26it/s, loss=0.115] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train Loss: 0.1809, Val Loss: 13.8514, Val Accuracy: 0.3972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|██████████| 691/691 [00:27<00:00, 25.21it/s, loss=0.376] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train Loss: 0.1739, Val Loss: 13.8613, Val Accuracy: 0.3983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|██████████| 691/691 [00:27<00:00, 25.24it/s, loss=0.123] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train Loss: 0.1695, Val Loss: 13.9006, Val Accuracy: 0.3951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|██████████| 691/691 [00:27<00:00, 25.10it/s, loss=0.162] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Train Loss: 0.1595, Val Loss: 14.2229, Val Accuracy: 0.3933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|██████████| 691/691 [00:27<00:00, 25.23it/s, loss=0.201] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Train Loss: 0.1581, Val Loss: 14.2968, Val Accuracy: 0.3899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|██████████| 691/691 [00:27<00:00, 25.29it/s, loss=0.175] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Train Loss: 0.1547, Val Loss: 14.4539, Val Accuracy: 0.3905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: 100%|██████████| 691/691 [00:27<00:00, 25.23it/s, loss=0.113] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Train Loss: 0.1500, Val Loss: 14.4469, Val Accuracy: 0.3919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 100%|██████████| 691/691 [00:27<00:00, 25.14it/s, loss=0.195] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Train Loss: 0.1464, Val Loss: 14.5270, Val Accuracy: 0.3871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 100%|██████████| 691/691 [00:27<00:00, 25.32it/s, loss=0.171] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Train Loss: 0.1427, Val Loss: 14.5643, Val Accuracy: 0.3800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 100%|██████████| 691/691 [00:27<00:00, 25.12it/s, loss=0.251] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Train Loss: 0.1424, Val Loss: 14.6361, Val Accuracy: 0.3903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30: 100%|██████████| 691/691 [00:27<00:00, 25.20it/s, loss=0.231] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Train Loss: 0.1425, Val Loss: 14.7243, Val Accuracy: 0.3924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30: 100%|██████████| 691/691 [00:27<00:00, 25.46it/s, loss=0.0931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Train Loss: 0.1369, Val Loss: 14.8427, Val Accuracy: 0.3983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30: 100%|██████████| 691/691 [00:27<00:00, 25.35it/s, loss=0.14]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Train Loss: 0.1382, Val Loss: 14.9455, Val Accuracy: 0.3880\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇▇▇█████████▇████████▇▇███▇</td></tr><tr><td>val_loss</td><td>▁▂▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>language</td><td>hi</td></tr><tr><td>train_loss</td><td>0.13825</td></tr><tr><td>val_accuracy</td><td>0.38802</td></tr><tr><td>val_loss</td><td>14.94555</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM_lang_hi_e32_h512</strong> at: <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/ugfbcbw4' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/ugfbcbw4</a><br> View project at: <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq</a><br>Synced 5 W&B file(s), 0 media file(s), 16 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250522_121443-ugfbcbw4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from config import Config\n",
    "# from dataloader import get_dataloader\n",
    "# from seq2seq import Seq2Seq\n",
    "# from utils import save_checkpoint, create_directory, indices_to_string\n",
    "\n",
    "def train(config, run_name=None, run_already_initialized=False):\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize W&B only if it's not already initialized\n",
    "    if not run_already_initialized:\n",
    "        if run_name:\n",
    "            wandb.init(project=config.wandb_project, entity=config.wandb_entity, name=run_name, config=vars(config))\n",
    "        else:\n",
    "            # Include language in the run name if not provided\n",
    "            default_run_name = f\"{config.cell_type}_lang_{config.language}_e{config.embed_size}_h{config.hidden_size}\"\n",
    "            wandb.init(project=config.wandb_project, entity=config.wandb_entity, name=default_run_name, config=vars(config))\n",
    "    \n",
    "        # Explicitly log the language being used\n",
    "        wandb.config.update({\"language\": config.language})\n",
    "    \n",
    "    print(f\"\\nTraining transliteration model for language: {config.language}\")\n",
    "    print(f\"Using dataset from: {config.data_dir}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        train_loader, train_dataset = get_dataloader(\n",
    "            config.train_file, \n",
    "            config.batch_size, \n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_loader, val_dataset = get_dataloader(\n",
    "            config.val_file, \n",
    "            config.batch_size, \n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        model = Seq2Seq(\n",
    "            input_size=train_dataset.source_vocab_size,\n",
    "            output_size=train_dataset.target_vocab_size,\n",
    "            embed_size=config.embed_size,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_encoder_layers=config.num_encoder_layers,\n",
    "            num_decoder_layers=config.num_decoder_layers,\n",
    "            dropout=config.dropout,\n",
    "            cell_type=config.cell_type\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "        \n",
    "        # Create directory for saving models\n",
    "        create_directory(config.model_dir)\n",
    "        \n",
    "        best_accuracy = 0.0\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(config.epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config.epochs}')\n",
    "            \n",
    "            for batch_idx, batch in enumerate(pbar):\n",
    "                # Move tensors to device\n",
    "                source = batch['source'].to(device)\n",
    "                target = batch['target'].to(device)\n",
    "                source_lengths = batch['source_lengths']\n",
    "                \n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                output = model(source, target, source_lengths, config.teacher_forcing_ratio)\n",
    "                \n",
    "                # Calculate loss\n",
    "                # Reshape output and target for loss calculation\n",
    "                output_dim = output.shape[-1]\n",
    "                output = output[:, 1:].reshape(-1, output_dim)  # Remove SOS token\n",
    "                target = target[:, 1:].reshape(-1)  # Remove SOS token\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Clip gradients\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "            # Calculate average loss\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            model.eval()\n",
    "            val_loss, val_accuracy = evaluate(model, val_loader, criterion, device, train_dataset)\n",
    "            \n",
    "            # Log metrics to W&B\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': avg_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'language': config.language  # Log language with metrics\n",
    "            })\n",
    "            \n",
    "            print(f'Epoch: {epoch+1}, Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "            \n",
    "            # Save model if it's the best so far\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "                if not os.path.exists(os.path.join(config.model_dir)):\n",
    "                    os.makedirs(os.path.join(config.model_dir))\n",
    "\n",
    "                # Save locally first with language code in filename\n",
    "                model_path = os.path.join(config.model_dir, f'best_model_{config.language}_{wandb.run.id}.pt')\n",
    "                save_checkpoint(\n",
    "                    model, \n",
    "                    optimizer, \n",
    "                    epoch, \n",
    "                    val_accuracy, \n",
    "                    model_path\n",
    "                )\n",
    "                \n",
    "                # Save model state dictionary to W&B\n",
    "                try:\n",
    "                    # Save model for W&B\n",
    "                    wandb_model_path = os.path.join(wandb.run.dir, \"best_model.pt\")\n",
    "                    torch.save(model.state_dict(), wandb_model_path)\n",
    "                    \n",
    "                    # Use W&B Artifact API instead of direct file save\n",
    "                    artifact = wandb.Artifact(f\"model-{config.language}-{wandb.run.id}\", type=\"model\")\n",
    "                    artifact.add_file(wandb_model_path)\n",
    "                    wandb.log_artifact(artifact)\n",
    "                    \n",
    "                    print(f\"Model successfully saved to W&B as artifact\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to save model to W&B: {str(e)}\")\n",
    "                    print(f\"Model was saved locally to {model_path}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                checkpoint_path = os.path.join(config.model_dir, f'checkpoint_{config.language}_{epoch+1}_{wandb.run.id}.pt')\n",
    "                save_checkpoint(\n",
    "                    model, \n",
    "                    optimizer, \n",
    "                    epoch, \n",
    "                    val_accuracy, \n",
    "                    checkpoint_path\n",
    "                )\n",
    "\n",
    "        # Only finish wandb if we started it in this function\n",
    "        if not run_already_initialized:\n",
    "            wandb.finish()\n",
    "        \n",
    "        return model, best_accuracy\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {str(e)}\")\n",
    "        # Only finish wandb if we started it in this function\n",
    "        if not run_already_initialized:\n",
    "            wandb.finish()\n",
    "        # Re-raise the exception for proper error handling\n",
    "        raise\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, dataset):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move tensors to device\n",
    "            source = batch['source'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "            source_lengths = batch['source_lengths']\n",
    "            target_lengths = batch['target_lengths']\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(source, target, source_lengths, teacher_forcing_ratio=0)\n",
    "            \n",
    "            # Calculate loss\n",
    "            output_dim = output.shape[-1]\n",
    "            output_for_loss = output[:, 1:].reshape(-1, output_dim)  # Remove SOS token\n",
    "            target_for_loss = target[:, 1:].reshape(-1)  # Remove SOS token\n",
    "            \n",
    "            loss = criterion(output_for_loss, target_for_loss)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Generate predictions\n",
    "            predictions = model.predict(\n",
    "                source,\n",
    "                source_lengths,\n",
    "                dataset.target_vocab_size,\n",
    "                dataset.target_char_to_idx['< SOS >'],\n",
    "                dataset.target_char_to_idx['<EOS>']\n",
    "            )\n",
    "            \n",
    "            # Calculate accuracy (exact match)\n",
    "            for i in range(len(predictions)):\n",
    "                pred_seq = [idx.item() for idx in predictions[i]]\n",
    "                pred_str = indices_to_string(pred_seq, dataset.target_idx_to_char, dataset.target_char_to_idx['<EOS>'])\n",
    "                target_str = batch['target_texts'][i]\n",
    "                \n",
    "                if pred_str == target_str:\n",
    "                    total_correct += 1\n",
    "            \n",
    "            total_examples += len(predictions)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_examples\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config()\n",
    "    train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T13:52:05.982740Z",
     "iopub.status.busy": "2025-05-22T13:52:05.981993Z",
     "iopub.status.idle": "2025-05-22T13:52:06.081505Z",
     "shell.execute_reply": "2025-05-22T13:52:06.080603Z",
     "shell.execute_reply.started": "2025-05-22T13:52:05.982717Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "# from config import Config\n",
    "# from dataloader import get_dataloader\n",
    "# from seq2seq import Seq2Seq\n",
    "# from utils import indices_to_string, create_directory\n",
    "\n",
    "def evaluate_model(model, dataloader, dataset, device, config):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            # Move tensors to device\n",
    "            source = batch['source'].to(device)\n",
    "            source_lengths = batch['source_lengths']\n",
    "            source_texts = batch['source_texts']\n",
    "            target_texts = batch['target_texts']\n",
    "            \n",
    "            # Generate predictions\n",
    "            predictions = model.predict(\n",
    "                source,\n",
    "                source_lengths,\n",
    "                dataset.target_vocab_size,\n",
    "                dataset.target_char_to_idx['< SOS >'],\n",
    "                dataset.target_char_to_idx['<EOS>']\n",
    "            )\n",
    "            \n",
    "            # Process each example in the batch\n",
    "            for i in range(len(predictions)):\n",
    "                pred_seq = [idx.item() for idx in predictions[i]]\n",
    "                pred_str = indices_to_string(pred_seq, dataset.target_idx_to_char, dataset.target_char_to_idx['<EOS>'])\n",
    "                target_str = batch['target_texts'][i]\n",
    "\n",
    "                # Check if prediction is correct\n",
    "                is_correct = pred_str == target_str\n",
    "                if is_correct:\n",
    "                    total_correct += 1\n",
    "                \n",
    "                # Store result\n",
    "                results.append({\n",
    "                    'source': source_texts[i],\n",
    "                    'target': target_texts[i],\n",
    "                    'prediction': pred_str,\n",
    "                    'correct': is_correct\n",
    "                })\n",
    "                \n",
    "            total_examples = len(predictions)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = total_correct / total_examples\n",
    "    \n",
    "    return results, accuracy\n",
    "\n",
    "def get_best_model_from_sweep(config, device, sweep_id=None):\n",
    "    \"\"\"Get the best model configuration from a completed sweep\"\"\"\n",
    "    \n",
    "    # Initialize W&B API\n",
    "    api = wandb.Api()\n",
    "    \n",
    "    # Get best run from the project based on validation accuracy\n",
    "    try:\n",
    "        if sweep_id is not None:\n",
    "            sweep = api.sweep(f\"{config.wandb_entity}/{config.wandb_project}/{sweep_id}\")\n",
    "        else:\n",
    "            print('No sweep ID provided. Please provide a valid sweep ID.')\n",
    "            raise ValueError(\"Sweep ID is required to find the best model in a sweep.\")\n",
    "        \n",
    "        best_val_acc = -1\n",
    "        best_run = None\n",
    "        \n",
    "        for run in sweep.runs:\n",
    "            val_acc = run.summary.get(\"val_accuracy\")\n",
    "            if val_acc is not None and val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_run = run\n",
    "        \n",
    "        if best_run is None:\n",
    "            raise ValueError(\"No runs with 'val_accuracy' found in the sweep\")\n",
    "\n",
    "        # Extract hyperparameters from the best run\n",
    "        best_config = {\n",
    "            'embed_size': best_run.config.get('embed_size', config.embed_size),\n",
    "            'hidden_size': best_run.config.get('hidden_size', config.hidden_size),\n",
    "            'num_encoder_layers': best_run.config.get('num_encoder_layers', config.num_encoder_layers),\n",
    "            'num_decoder_layers': best_run.config.get('num_decoder_layers', config.num_decoder_layers),\n",
    "            'cell_type': best_run.config.get('cell_type', config.cell_type),\n",
    "            'dropout': best_run.config.get('dropout', config.dropout)\n",
    "        }\n",
    "        \n",
    "        # Update config with best hyperparameters\n",
    "        for key, value in best_config.items():\n",
    "            setattr(config, key, value)\n",
    "        \n",
    "        print(\"Best model hyperparameters:\")\n",
    "        for key, value in best_config.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        # Try to download the best model\n",
    "        try:\n",
    "            # Try to find artifact by name\n",
    "            artifact = api.artifact(f\"{config.wandb_entity}/{config.wandb_project}/model-{config.language}-{best_run.id}:latest\")\n",
    "            artifact_dir = artifact.download()\n",
    "            best_model_path = os.path.join(artifact_dir, \"best_model.pt\")\n",
    "            \n",
    "            # If artifact download failed, try to get the file directly\n",
    "            if not os.path.exists(best_model_path):\n",
    "                best_model_path = best_run.file(\"best_model.pt\").download()\n",
    "            \n",
    "            print(f\"Downloaded best model from W&B\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download model from W&B: {str(e)}\")\n",
    "            # Look for local model with matching run ID\n",
    "            model_dir = config.model_dir\n",
    "            model_files = [f for f in os.listdir(model_dir) if f.startswith(f'best_model_{config.language}_{best_run.id}')]\n",
    "            \n",
    "            if model_files:\n",
    "                best_model_path = os.path.join(model_dir, model_files[0])\n",
    "                print(f\"Using local model: {best_model_path}\")\n",
    "            else:\n",
    "                # If no matching model, use the most recent local model\n",
    "                model_files = [f for f in os.listdir(model_dir) if f.startswith(f'best_model_{config.language}_')]\n",
    "                if model_files:\n",
    "                    model_files.sort(key=lambda x: os.path.getmtime(os.path.join(model_dir, x)), reverse=True)\n",
    "                    best_model_path = os.path.join(model_dir, model_files[0])\n",
    "                    print(f\"Using most recent local model instead: {best_model_path}\")\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"No model file found for language '{config.language}'. Please train a model first.\")\n",
    "        \n",
    "        return best_config, best_model_path\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error finding best model: {str(e)}\")\n",
    "        print(\"Looking for local model instead...\")\n",
    "        \n",
    "        # Look for local model\n",
    "        model_dir = config.model_dir\n",
    "        if not os.path.exists(model_dir):\n",
    "            raise FileNotFoundError(f\"Model directory '{model_dir}' not found\")\n",
    "            \n",
    "        model_files = [f for f in os.listdir(model_dir) if f.startswith(f'best_model_{config.language}_')]\n",
    "        \n",
    "        if model_files:\n",
    "            # Use the most recent model file\n",
    "            model_files.sort(key=lambda x: os.path.getmtime(os.path.join(model_dir, x)), reverse=True)\n",
    "            best_model_path = os.path.join(model_dir, model_files[0])\n",
    "            print(f\"Using local model: {best_model_path}\")\n",
    "            # Keep the default config\n",
    "            best_config = {\n",
    "                'embed_size': config.embed_size,\n",
    "                'hidden_size': config.hidden_size,\n",
    "                'num_encoder_layers': config.num_encoder_layers,\n",
    "                'num_decoder_layers': config.num_decoder_layers,\n",
    "                'cell_type': config.cell_type,\n",
    "                'dropout': config.dropout\n",
    "            }\n",
    "            return best_config, best_model_path\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No model file found for language '{config.language}'. Please train a model first.\")\n",
    "\n",
    "def create_prediction_visualizations(results, config):\n",
    "    \"\"\"Create visualizations for the prediction results\"\"\"\n",
    "    \n",
    "    # Try to use a font that supports Devanagari if available\n",
    "    # First, attempt to find a suitable font\n",
    "    font_found = False\n",
    "    \n",
    "    # List of potential fonts that might support Devanagari\n",
    "    potential_fonts = ['Nirmala UI', 'Mangal', 'Arial Unicode MS', 'FreeSerif', 'Noto Sans Devanagari']\n",
    "    \n",
    "    for font in potential_fonts:\n",
    "        try:\n",
    "            # Check if the font is available\n",
    "            test_prop = FontProperties(family=font)\n",
    "            if test_prop.get_family()[0] == font:\n",
    "                # Set this font as the default\n",
    "                plt.rcParams['font.family'] = font\n",
    "                print(f\"Using font: {font} for Devanagari support\")\n",
    "                font_found = True\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if not font_found:\n",
    "        print(\"Warning: No font with Devanagari support found. Text may not display correctly.\")\n",
    "        # Use a fallback approach - encode special characters\n",
    "        def sanitize_text(text):\n",
    "            # Replace Devanagari characters with their Unicode code point representation\n",
    "            return ''.join([f\"U+{ord(c):04X}\" if ord(c) > 255 else c for c in text])\n",
    "    else:\n",
    "        # If font is found, we can use original text\n",
    "        def sanitize_text(text):\n",
    "            return text\n",
    "    \n",
    "    # Create a results DataFrame for easier analysis\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Apply sanitization to text columns\n",
    "    df['source_display'] = df['source'].apply(sanitize_text)\n",
    "    df['target_display'] = df['target'].apply(sanitize_text)\n",
    "    df['prediction_display'] = df['prediction'].apply(sanitize_text)\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    accuracy = df['correct'].mean()\n",
    "    total_samples = len(df)\n",
    "    correct_samples_count = df['correct'].sum()\n",
    "    \n",
    "    # Handle possible empty groups\n",
    "    correct_df = df[df['correct'] == True]\n",
    "    incorrect_df = df[df['correct'] == False]\n",
    "    \n",
    "    # Sample safely - ensuring we don't try to sample more items than available\n",
    "    correct_sample_size = min(5, len(correct_df))\n",
    "    incorrect_sample_size = min(5, len(incorrect_df))\n",
    "    \n",
    "    # Sample from each group\n",
    "    correct_samples = correct_df.sample(correct_sample_size) if correct_sample_size > 0 else pd.DataFrame()\n",
    "    incorrect_samples = incorrect_df.sample(incorrect_sample_size) if incorrect_sample_size > 0 else pd.DataFrame()\n",
    "    \n",
    "    # Combine samples\n",
    "    samples = pd.concat([correct_samples, incorrect_samples])\n",
    "    \n",
    "    # If we have samples to display, shuffle them\n",
    "    if len(samples) > 0:\n",
    "        samples = samples.sample(frac=1)  # Shuffle the combined samples\n",
    "    \n",
    "    # Determine the number of plots to display\n",
    "    n_samples = len(samples)\n",
    "    \n",
    "    if n_samples == 0:\n",
    "        # Handle the case where there are no samples\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "        ax.text(0.5, 0.5, \"No samples available for visualization\", \n",
    "                horizontalalignment='center', verticalalignment='center')\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        # Create a grid for the samples\n",
    "        rows = min(5, max(1, (n_samples + 1) // 2))  # At least 1 row, at most 5 rows\n",
    "        cols = min(2, n_samples)  # At most 2 columns\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(16, 10))\n",
    "        \n",
    "        # Handle the case of a single subplot\n",
    "        if rows == 1 and cols == 1:\n",
    "            axes = np.array([axes])\n",
    "        \n",
    "        # Flatten axes array for easier indexing\n",
    "        axes = np.array(axes).flatten()\n",
    "        \n",
    "        # Plot each sample\n",
    "        for i, (idx, sample) in enumerate(samples.iterrows()):\n",
    "            if i >= len(axes):\n",
    "                break\n",
    "                \n",
    "            source = sample['source_display']  # Use sanitized text\n",
    "            target = sample['target_display']  # Use sanitized text\n",
    "            prediction = sample['prediction_display']  # Use sanitized text\n",
    "            correct = sample['correct']\n",
    "            \n",
    "            bg_color = \"#d0f0d0\" if correct else \"#f0d0d0\"\n",
    "            title_color = \"green\" if correct else \"red\"\n",
    "            \n",
    "            # Create a table visualization but avoid displaying directly\n",
    "            # Instead, create a text display\n",
    "            ax = axes[i]\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Create text display instead of table\n",
    "            text_content = f\"Source: {source}\\nTarget: {target}\\nPrediction: {prediction}\"\n",
    "            \n",
    "            # Display text with background color for prediction\n",
    "            ax.text(0.5, 0.5, text_content, \n",
    "                    horizontalalignment='center', \n",
    "                    verticalalignment='center',\n",
    "                    transform=ax.transAxes,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.5\", \n",
    "                             facecolor=bg_color if correct else \"#f0d0d0\",\n",
    "                             alpha=0.5))\n",
    "            \n",
    "            ax.set_title(f\"Sample {i+1}: {'Correct' if correct else 'Incorrect'}\", color=title_color)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for j in range(i+1, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "            axes[j].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure with a more basic approach\n",
    "    visualization_path = os.path.join(config.prediction_dir, 'prediction_grid.png')\n",
    "    plt.savefig(visualization_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create error analysis plots\n",
    "    \n",
    "    # 1. Correct vs Incorrect counts\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Fixed: Use hue parameter instead of direct palette assignment\n",
    "    sns.countplot(x='correct', hue='correct', data=df, palette=[\"#f0d0d0\", \"#d0f0d0\"], legend=False)\n",
    "    plt.title(f\"Prediction Results: {correct_samples_count} Correct / {total_samples} Total ({accuracy:.2%})\")\n",
    "    plt.xlabel(\"Prediction Correctness\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks([0, 1], [\"Incorrect\", \"Correct\"])\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.tight_layout()\n",
    "    error_count_path = os.path.join(config.prediction_dir, 'correct_vs_incorrect.png')\n",
    "    plt.savefig(error_count_path)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Error analysis by string length\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Add string length to DataFrame\n",
    "    df['source_length'] = df['source'].apply(len)\n",
    "    df['accuracy_by_length'] = df.groupby('source_length')['correct'].transform('mean')\n",
    "    \n",
    "    # Group by length\n",
    "    length_stats = df.groupby('source_length').agg(\n",
    "        count=('correct', 'count'),\n",
    "        accuracy=('correct', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Create the plot - only include lengths with enough samples\n",
    "    # Get lengths with at least 3 samples or 1% of the dataset\n",
    "    min_count = max(3, len(df) * 0.01)\n",
    "    length_stats_filtered = length_stats[length_stats['count'] >= min_count]\n",
    "    \n",
    "    if len(length_stats_filtered) > 0:\n",
    "        # Plot accuracy by length\n",
    "        plt.bar(length_stats_filtered['source_length'], length_stats_filtered['accuracy'], \n",
    "                color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        plt.xlabel('Source Word Length')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy by Word Length')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add count labels\n",
    "        for idx, row in length_stats_filtered.iterrows():\n",
    "            plt.text(row['source_length'], row['accuracy'] + 0.02, \n",
    "                    f\"n={row['count']}\", ha='center', fontsize=8)\n",
    "        \n",
    "        # Improve appearance\n",
    "        plt.ylim(0, 1.1)\n",
    "    else:\n",
    "        # Handle case with no data\n",
    "        plt.text(0.5, 0.5, \"Insufficient data for length analysis\", \n",
    "                horizontalalignment='center', verticalalignment='center',\n",
    "                transform=plt.gca().transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    length_analysis_path = os.path.join(config.prediction_dir, 'accuracy_by_length.png')\n",
    "    plt.savefig(length_analysis_path)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Character-level error analysis\n",
    "    incorrect_df = df[df['correct'] == False].copy()  # Fixed: Create a copy to avoid SettingWithCopyWarning\n",
    "    \n",
    "    char_error_path = None\n",
    "    # Check if there are any incorrect predictions for analysis\n",
    "    if len(incorrect_df) > 0:\n",
    "        # Function to find character-level differences - use sanitized text\n",
    "        def get_char_differences(row):\n",
    "            target = row['target']\n",
    "            pred = row['prediction']\n",
    "            \n",
    "            # Simple character error analysis\n",
    "            min_len = min(len(target), len(pred))\n",
    "            errors = []\n",
    "            \n",
    "            for i in range(min_len):\n",
    "                if target[i] != pred[i]:\n",
    "                    errors.append((i, target[i], pred[i]))\n",
    "            \n",
    "            # Handle length differences\n",
    "            if len(target) > len(pred):\n",
    "                for i in range(min_len, len(target)):\n",
    "                    errors.append((i, target[i], \"\"))\n",
    "            elif len(pred) > len(target):\n",
    "                for i in range(min_len, len(pred)):\n",
    "                    errors.append((i, \"\", pred[i]))\n",
    "                    \n",
    "            return errors\n",
    "        \n",
    "        # Apply function to get character differences\n",
    "        incorrect_df['char_errors'] = incorrect_df.apply(get_char_differences, axis=1)\n",
    "        \n",
    "        # Extract all character errors\n",
    "        all_errors = []\n",
    "        for errors in incorrect_df['char_errors']:\n",
    "            for pos, target_char, pred_char in errors:\n",
    "                # Use sanitized characters for display\n",
    "                t_char = sanitize_text(target_char)\n",
    "                p_char = sanitize_text(pred_char)\n",
    "                all_errors.append((t_char, p_char))\n",
    "        \n",
    "        # Count pair occurrences\n",
    "        error_counts = Counter(all_errors)\n",
    "        \n",
    "        # Get the top 15 most common errors\n",
    "        top_errors = error_counts.most_common(15)\n",
    "        \n",
    "        if top_errors:\n",
    "            # Create a DataFrame for visualization\n",
    "            error_df = pd.DataFrame(top_errors, columns=['char_pair', 'count'])\n",
    "            error_df['target_char'] = error_df['char_pair'].apply(lambda x: x[0] if x[0] else '(empty)')  # Fixed: Use \"(empty)\" instead of \"∅\"\n",
    "            error_df['pred_char'] = error_df['char_pair'].apply(lambda x: x[1] if x[1] else '(empty)')  # Fixed: Use \"(empty)\" instead of \"∅\"\n",
    "            error_df['pair_label'] = error_df.apply(lambda row: f\"{row['target_char']} -> {row['pred_char']}\", axis=1)  # Fixed: Use \"->\" instead of \"→\"\n",
    "            \n",
    "            plt.figure(figsize=(14, 8))\n",
    "            \n",
    "            # Use a horizontal bar chart for better readability\n",
    "            bars = plt.barh(error_df['pair_label'], error_df['count'], color='salmon', edgecolor='darkred')\n",
    "            plt.xlabel('Count')\n",
    "            plt.ylabel('Character Error (Target -> Prediction)')  # Fixed: Use \"->\" instead of \"→\"\n",
    "            plt.title('Most Common Character-Level Errors')\n",
    "            plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "            \n",
    "            # Add count labels\n",
    "            for bar in bars:\n",
    "                width = bar.get_width()\n",
    "                plt.text(width + 0.1, bar.get_y() + bar.get_height()/2, \n",
    "                        f'{width}', va='center', fontsize=9)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save the figure\n",
    "            char_error_path = os.path.join(config.prediction_dir, 'character_errors.png')\n",
    "            plt.savefig(char_error_path)\n",
    "            plt.close()\n",
    "    \n",
    "    return {\n",
    "        'prediction_grid': visualization_path,\n",
    "        'error_count': error_count_path,\n",
    "        'length_analysis': length_analysis_path,\n",
    "        'char_error': char_error_path\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Run evaluation for best model in a sweep')\n",
    "    parser.add_argument('--sweep_id', type=str, help='The sweep ID to use (if already created)')\n",
    "\n",
    "    config = Config()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    sweep_id = 'v0f3rkuy'#args.sweep_id\n",
    "\n",
    "    # Create vanilla prediction directory specifically for Question 4\n",
    "    vanilla_pred_dir = os.path.join(os.path.dirname(config.prediction_dir), \"predictions_vanilla\")\n",
    "    if not os.path.exists(vanilla_pred_dir):\n",
    "        os.makedirs(vanilla_pred_dir, exist_ok=True)\n",
    "    \n",
    "    # Temporarily override the prediction directory\n",
    "    original_pred_dir = config.prediction_dir\n",
    "    config.prediction_dir = vanilla_pred_dir\n",
    "    \n",
    "    # Initialize W&B for evaluation\n",
    "    wandb.init(project=config.wandb_project, entity=config.wandb_entity, job_type=\"evaluation\", \n",
    "               name=f\"vanilla_eval_{config.language}\")\n",
    "    \n",
    "    # Log the language being evaluated\n",
    "    wandb.config.update({\"language\": config.language})\n",
    "    \n",
    "    # Load test data\n",
    "    test_loader, test_dataset = get_dataloader(\n",
    "        config.test_file, \n",
    "        config.batch_size, \n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Get the best model from sweep\n",
    "    best_config, best_model_path = get_best_model_from_sweep(config, device, sweep_id)\n",
    "\n",
    "    # Load the model checkpoint to get the correct vocabulary size\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "\n",
    "    # Extract the vocabulary size from the model checkpoint\n",
    "    output_size = checkpoint['decoder.fc_out.bias'].size(0)  # This should be 66 based on the error\n",
    "\n",
    "    # Create model with the best configuration and correct vocabulary size\n",
    "    model = Seq2Seq(\n",
    "        input_size=test_dataset.source_vocab_size,\n",
    "        output_size=output_size,  # Use the output size from the checkpoint instead\n",
    "        embed_size=best_config['embed_size'],\n",
    "        hidden_size=best_config['hidden_size'],\n",
    "        num_encoder_layers=best_config['num_encoder_layers'],\n",
    "        num_decoder_layers=best_config['num_decoder_layers'],\n",
    "        dropout=best_config['dropout'],\n",
    "        cell_type=best_config['cell_type']\n",
    "    )\n",
    "    \n",
    "    # Load the model weights\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create prediction directory\n",
    "    create_directory(config.prediction_dir)\n",
    "    \n",
    "    # Evaluate model\n",
    "    results, accuracy = evaluate_model(model, test_loader, test_dataset, device, config)\n",
    "    \n",
    "    # Create visualizations\n",
    "    visualization_paths = create_prediction_visualizations(results, config)\n",
    "    \n",
    "    # Save results\n",
    "    output_file = os.path.join(config.prediction_dir, f\"predictions_{config.language}_{wandb.run.id}.json\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Log accuracy to W&B\n",
    "    wandb.log({'test_accuracy': accuracy, 'language': config.language})\n",
    "    \n",
    "    print(f'Test Accuracy for {config.language}: {accuracy:.4f}')\n",
    "    print(f'Predictions saved to {output_file}')\n",
    "    \n",
    "    # Create a table for W&B\n",
    "    columns = [\"source\", \"target\", \"prediction\", \"correct\"]\n",
    "    data = [[r[\"source\"], r[\"target\"], r[\"prediction\"], r[\"correct\"]] for r in results]\n",
    "    table = wandb.Table(columns=columns, data=data)\n",
    "    wandb.log({\"predictions\": table})\n",
    "    \n",
    "    # Log the visualization as W&B images\n",
    "    for name, path in visualization_paths.items():\n",
    "        if path and os.path.exists(path):\n",
    "            image = wandb.Image(path)\n",
    "            wandb.log({name: image})\n",
    "            print(f\"logged {name} to W&B from {path}\")\n",
    "    \n",
    "    # Log some sample predictions\n",
    "    samples = results[:10]  # First 10 samples\n",
    "    sample_data = \"\\n\".join([f\"Source: {s['source']}, Target: {s['target']}, Prediction: {s['prediction']}, Correct: {s['correct']}\" for s in samples])\n",
    "    wandb.log({\"samples\": wandb.Html(sample_data)})\n",
    "    \n",
    "    # Analyze error patterns\n",
    "    error_analysis = analyze_errors(results)\n",
    "    wandb.log({\"error_analysis\": wandb.Html(error_analysis)})\n",
    "    \n",
    "    # Restore original prediction directory\n",
    "    config.prediction_dir = original_pred_dir\n",
    "    \n",
    "    wandb.finish()\n",
    "\n",
    "def analyze_errors(results):\n",
    "    \"\"\"Analyze common error patterns in the predictions\"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Filter for incorrect predictions\n",
    "    incorrect = df[df['correct'] == False].copy()  # Fixed: Create a copy to avoid SettingWithCopyWarning\n",
    "    \n",
    "    if len(incorrect) == 0:\n",
    "        return \"<p>No errors to analyze!</p>\"\n",
    "    \n",
    "    # Error patterns analysis\n",
    "    analysis = \"<h3>Error Analysis</h3>\"\n",
    "    \n",
    "    # 1. Length differences\n",
    "    incorrect.loc[:, 'target_len'] = incorrect['target'].apply(len)  # Fixed: Use .loc to avoid SettingWithCopyWarning\n",
    "    incorrect.loc[:, 'pred_len'] = incorrect['prediction'].apply(len)  # Fixed: Use .loc to avoid SettingWithCopyWarning\n",
    "    incorrect.loc[:, 'len_diff'] = incorrect['pred_len'] - incorrect['target_len']  # Fixed: Use .loc to avoid SettingWithCopyWarning\n",
    "    \n",
    "    len_diff_counts = incorrect['len_diff'].value_counts().head(5)\n",
    "    len_diff_html = \"<h4>Length Differences in Incorrect Predictions</h4>\"\n",
    "    len_diff_html += \"<ul>\"\n",
    "    \n",
    "    for diff, count in len_diff_counts.items():\n",
    "        len_diff_html += f\"<li>Difference of {diff} characters: {count} occurrences ({(count/len(incorrect))*100:.1f}%)</li>\"\n",
    "    \n",
    "    len_diff_html += \"</ul>\"\n",
    "    analysis += len_diff_html\n",
    "    \n",
    "    # 2. Example error cases for inspection\n",
    "    analysis += \"<h4>Sample Error Cases</h4>\"\n",
    "    analysis += \"<table border='1' style='border-collapse: collapse;'>\"\n",
    "    analysis += \"<tr><th>Source</th><th>Target</th><th>Prediction</th><th>Analysis</th></tr>\"\n",
    "    \n",
    "    # Select diverse error examples\n",
    "    error_samples = []\n",
    "    \n",
    "    # Shorter prediction\n",
    "    shorter_pred = incorrect[incorrect['len_diff'] < 0].head(1)\n",
    "    if not shorter_pred.empty:\n",
    "        error_samples.append(shorter_pred.iloc[0])\n",
    "    \n",
    "    # Longer prediction\n",
    "    longer_pred = incorrect[incorrect['len_diff'] > 0].head(1)\n",
    "    if not longer_pred.empty:\n",
    "        error_samples.append(longer_pred.iloc[0])\n",
    "    \n",
    "    # Same length but incorrect\n",
    "    same_len = incorrect[incorrect['len_diff'] == 0].head(1)\n",
    "    if not same_len.empty:\n",
    "        error_samples.append(same_len.iloc[0])\n",
    "    \n",
    "    # Add more examples if needed\n",
    "    if len(error_samples) < 3:\n",
    "        more_examples = incorrect.head(3 - len(error_samples))\n",
    "        error_samples.extend(more_examples.to_dict('records'))\n",
    "    \n",
    "    for sample in error_samples:\n",
    "        source = sample['source']\n",
    "        target = sample['target']\n",
    "        prediction = sample['prediction']\n",
    "        \n",
    "        # Simple error analysis\n",
    "        analysis_text = \"\"\n",
    "        \n",
    "        if len(prediction) < len(target):\n",
    "            analysis_text = f\"Missing characters (prediction too short by {len(target) - len(prediction)} chars)\"\n",
    "        elif len(prediction) > len(target):\n",
    "            analysis_text = f\"Extra characters (prediction too long by {len(prediction) - len(target)} chars)\"\n",
    "        else:\n",
    "            # Find positions of difference\n",
    "            diff_positions = []\n",
    "            for i, (t, p) in enumerate(zip(target, prediction)):\n",
    "                if t != p:\n",
    "                    diff_positions.append(i)\n",
    "            analysis_text = f\"Character mismatch at positions: {diff_positions}\"\n",
    "        \n",
    "        analysis += f\"<tr><td>{source}</td><td>{target}</td><td>{prediction}</td><td>{analysis_text}</td></tr>\"\n",
    "    \n",
    "    analysis += \"</table>\"\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7454609,
     "sourceId": 11863051,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
