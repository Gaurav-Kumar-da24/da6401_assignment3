{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11863011,"sourceType":"datasetVersion","datasetId":7454582}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:27:31.911510Z","iopub.execute_input":"2025-05-20T21:27:31.912356Z","iopub.status.idle":"2025-05-20T21:27:32.871558Z","shell.execute_reply.started":"2025-05-20T21:27:31.912329Z","shell.execute_reply":"2025-05-20T21:27:32.870739Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/dataset/dakshina_dataset_v1.0/README.md\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv/wiki-full.urls.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv/wiki-full.urls.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv/wiki-full.urls.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv/wiki-full.urls.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv/wiki-full.urls.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv/wiki-full.urls.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv/wiki-full.urls.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv/wiki-full.urls.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv/wiki-full.urls.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv/wiki-full.urls.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv/wiki-full.urls.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt/wiki-filt.valid.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv/wiki-full.nonblock.sections.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv/wiki-filt.train.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv/wiki-filt.valid.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt/wiki-full.omit_pages.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt/wiki-filt.train.text.shuf.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv/wiki-filt.valid.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv/wiki-full.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv/wiki-filt.train.text.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv/wiki-full.urls.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt/wiki-full.nonblock.sections.list.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv/wiki-full.info.sorted.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n/kaggle/input/dataset/dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport wandb\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom types import SimpleNamespace\nimport logging\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nimport time\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:27:32.873051Z","iopub.execute_input":"2025-05-20T21:27:32.873826Z","iopub.status.idle":"2025-05-20T21:27:37.111697Z","shell.execute_reply.started":"2025-05-20T21:27:32.873797Z","shell.execute_reply":"2025-05-20T21:27:37.110955Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"wandb.login(key=\"367b223a3f0954311018074733b3ed8dc409c2ff\")\n# Constants\nwandb_project = \"DL_A3_seq2seq\"\nwandb_entity= \"da24m006-iit-madras\"  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:27:37.112473Z","iopub.execute_input":"2025-05-20T21:27:37.112870Z","iopub.status.idle":"2025-05-20T21:27:44.046045Z","shell.execute_reply.started":"2025-05-20T21:27:37.112838Z","shell.execute_reply":"2025-05-20T21:27:44.045350Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda24m006\u001b[0m (\u001b[33mda24m006-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:27:44.046808Z","iopub.execute_input":"2025-05-20T21:27:44.047194Z","iopub.status.idle":"2025-05-20T21:27:44.113564Z","shell.execute_reply.started":"2025-05-20T21:27:44.047174Z","shell.execute_reply":"2025-05-20T21:27:44.112083Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"data_dir = '/kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons'\ntraindata = pd.read_csv(f'{data_dir}/hi.translit.sampled.train.tsv',sep='\\t',names=['native', 'latin', 'count'],usecols=[0, 1])\nvaldata= pd.read_csv(f'{data_dir}/hi.translit.sampled.dev.tsv',sep='\\t',names=['native', 'latin', 'count'],usecols=[0, 1])\ntestdata = pd.read_csv(f'{data_dir}/hi.translit.sampled.test.tsv',sep='\\t',names=['native', 'latin', 'count'],usecols=[0, 1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:27:44.116192Z","iopub.execute_input":"2025-05-20T21:27:44.116571Z","iopub.status.idle":"2025-05-20T21:27:44.223159Z","shell.execute_reply.started":"2025-05-20T21:27:44.116551Z","shell.execute_reply":"2025-05-20T21:27:44.222345Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"traindata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:27:44.224062Z","iopub.execute_input":"2025-05-20T21:27:44.224331Z","iopub.status.idle":"2025-05-20T21:27:44.246960Z","shell.execute_reply.started":"2025-05-20T21:27:44.224310Z","shell.execute_reply":"2025-05-20T21:27:44.246275Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"          native       latin\n0             अं          an\n1        अंकगणित    ankganit\n2           अंकल       uncle\n3          अंकुर       ankur\n4         अंकुरण     ankuran\n...          ...         ...\n44199  ह्वेनसांग  hiuentsang\n44200  ह्वेनसांग  hsuantsang\n44201  ह्वेनसांग    hyensang\n44202  ह्वेनसांग    xuanzang\n44203          ॐ          om\n\n[44204 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>native</th>\n      <th>latin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>अं</td>\n      <td>an</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>अंकगणित</td>\n      <td>ankganit</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>अंकल</td>\n      <td>uncle</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>अंकुर</td>\n      <td>ankur</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>अंकुरण</td>\n      <td>ankuran</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>44199</th>\n      <td>ह्वेनसांग</td>\n      <td>hiuentsang</td>\n    </tr>\n    <tr>\n      <th>44200</th>\n      <td>ह्वेनसांग</td>\n      <td>hsuantsang</td>\n    </tr>\n    <tr>\n      <th>44201</th>\n      <td>ह्वेनसांग</td>\n      <td>hyensang</td>\n    </tr>\n    <tr>\n      <th>44202</th>\n      <td>ह्वेनसांग</td>\n      <td>xuanzang</td>\n    </tr>\n    <tr>\n      <th>44203</th>\n      <td>ॐ</td>\n      <td>om</td>\n    </tr>\n  </tbody>\n</table>\n<p>44204 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"print(\"Total Dataset: \")\nprint(\"Total train data:\", len(traindata))\nprint(\"Total validation data:\", len(valdata))\nprint(\"Totat test data:\", len(testdata))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:27:44.247731Z","iopub.execute_input":"2025-05-20T21:27:44.247978Z","iopub.status.idle":"2025-05-20T21:27:44.252208Z","shell.execute_reply.started":"2025-05-20T21:27:44.247957Z","shell.execute_reply":"2025-05-20T21:27:44.251621Z"}},"outputs":[{"name":"stdout","text":"Total Dataset: \nTotal train data: 44204\nTotal validation data: 4358\nTotat test data: 4502\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"Hindi_vocab = set()\nmax_hin_len = 0\n\n# Process training data\nfor x in range(len(traindata)):\n    native_word = traindata.iloc[x]['native']\n    if pd.isna(native_word):\n        continue\n\n    max_hin_len = max(max_hin_len, len(native_word))\n    for char in native_word:\n        Hindi_vocab.add(char)\n\n# Process test data\nfor x in range(len(testdata)):\n    native_word = testdata.iloc[x]['native']\n    if pd.isna(native_word):\n        continue\n\n    for char in native_word:\n        if char not in Hindi_vocab:\n            print(char)\n            Hindi_vocab.add(char)\n\n\nHindi_vocab = sorted(Hindi_vocab)\n\nprint(\"Max Hindi word length:\", max_hin_len)\nprint(\"Total unique Hindi characters:\", len(Hindi_vocab))\nprint(Hindi_vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:27:44.253133Z","iopub.execute_input":"2025-05-20T21:27:44.253440Z","iopub.status.idle":"2025-05-20T21:27:45.179575Z","shell.execute_reply.started":"2025-05-20T21:27:44.253404Z","shell.execute_reply":"2025-05-20T21:27:45.178953Z"}},"outputs":[{"name":"stdout","text":"Max Hindi word length: 19\nTotal unique Hindi characters: 63\n['ँ', 'ं', 'ः', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ऑ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह', '़', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॅ', 'े', 'ै', 'ॉ', 'ो', 'ौ', '्', 'ॐ']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"max_eng_len = 0\nEnglish_vocab = set() \n\nfor x in range(len(traindata)):\n    latin_word = traindata.iloc[x]['latin']\n    \n    if pd.isna(latin_word):\n        continue  # skip if value is missing\n\n    word_len = len(latin_word)\n    max_eng_len = max(max_eng_len, word_len)\n\n    for char in latin_word:\n        English_vocab.add(char)\n\n# Convert set to sorted list\nEnglish_vocab = sorted(English_vocab)\nprint(\"Max english word length:\", max_eng_len)\nprint(\"Total unique english characters:\", len(English_vocab))\nprint(English_vocab)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:27:45.180236Z","iopub.execute_input":"2025-05-20T21:27:45.180549Z","iopub.status.idle":"2025-05-20T21:27:45.992708Z","shell.execute_reply.started":"2025-05-20T21:27:45.180531Z","shell.execute_reply":"2025-05-20T21:27:45.992030Z"}},"outputs":[{"name":"stdout","text":"Max english word length: 20\nTotal unique english characters: 26\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"Dataset Preprocessing","metadata":{}},{"cell_type":"code","source":"class TransliterationDataset(Dataset):\n    def __init__(self, file_path):\n        self.source_texts = []\n        self.target_texts = []\n        \n        # Check if file exists\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f\"Data file not found: {file_path}\")\n            \n        with open(file_path, 'r', encoding='utf-8') as f:\n            for line_num, line in enumerate(f, 1):\n                parts = line.strip().split('\\t')\n                \n                # Handle different file formats\n                if len(parts) == 3:  # Format: Hindi\\tRomanized\\tFrequency\n                    target, source, _ = parts\n                    self.source_texts.append(source)\n                    self.target_texts.append(target)\n                elif len(parts) == 2:  # Format: Hindi\\tRomanized\n                    target, source = parts\n                    self.source_texts.append(source)\n                    self.target_texts.append(target)\n                else:\n                    print(f\"Warning: Line {line_num} has unexpected format (expected 2 or 3 columns): {line.strip()}\")\n        \n        # If no data was loaded, raise an error\n        if len(self.source_texts) == 0:\n            raise ValueError(f\"No data was loaded from {file_path}. File may be empty or incorrectly formatted.\")\n                    \n        self.source_char_set = set()\n        self.target_char_set = set()\n        \n        for source in self.source_texts:\n            self.source_char_set.update(source)\n        \n        for target in self.target_texts:\n            self.target_char_set.update(target)\n        \n        self.source_char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted(self.source_char_set))}\n        self.source_idx_to_char = {idx + 1: char for idx, char in enumerate(sorted(self.source_char_set))}\n        self.source_char_to_idx['<PAD>'] = 0\n        self.source_idx_to_char[0] = '<PAD>'\n        self.source_char_to_idx['< SOS >'] = len(self.source_char_to_idx)\n        self.source_idx_to_char[len(self.source_idx_to_char)] = '< SOS >'\n        self.source_char_to_idx['<EOS>'] = len(self.source_char_to_idx)\n        self.source_idx_to_char[len(self.source_idx_to_char)] = '<EOS>'\n        \n        self.target_char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted(self.target_char_set))}\n        self.target_idx_to_char = {idx + 1: char for idx, char in enumerate(sorted(self.target_char_set))}\n        self.target_char_to_idx['<PAD>'] = 0\n        self.target_idx_to_char[0] = '<PAD>'\n        self.target_char_to_idx['< SOS >'] = len(self.target_char_to_idx)\n        self.target_idx_to_char[len(self.target_idx_to_char)] = '< SOS >'\n        self.target_char_to_idx['<EOS>'] = len(self.target_char_to_idx)\n        self.target_idx_to_char[len(self.target_idx_to_char)] = '<EOS>'\n        \n    def __len__(self):\n        return len(self.source_texts)\n    \n    def __getitem__(self, idx):\n        source = self.source_texts[idx]\n        target = self.target_texts[idx]\n        \n        source_indices = [self.source_char_to_idx['< SOS >']] + [self.source_char_to_idx[char] for char in source] + [self.source_char_to_idx['<EOS>']]\n        target_indices = [self.target_char_to_idx['< SOS >']] + [self.target_char_to_idx[char] for char in target] + [self.target_char_to_idx['<EOS>']]\n        \n        return {\n            'source': source,\n            'target': target,\n            'source_indices': source_indices,\n            'target_indices': target_indices,\n            'source_length': len(source_indices),\n            'target_length': len(target_indices)\n        }\n    \n    @property\n    def source_vocab_size(self):\n        return len(self.source_char_to_idx)\n    \n    @property\n    def target_vocab_size(self):\n        return len(self.target_char_to_idx)\n\ndef collate_fn(batch):\n    max_source_len = max([len(item['source_indices']) for item in batch])\n    max_target_len = max([len(item['target_indices']) for item in batch])\n    \n    source_batch = []\n    target_batch = []\n    source_lengths = []\n    target_lengths = []\n    \n    source_texts = []\n    target_texts = []\n    \n    for item in batch:\n        source_padded = item['source_indices'] + [0] * (max_source_len - len(item['source_indices']))\n        target_padded = item['target_indices'] + [0] * (max_target_len - len(item['target_indices']))\n        \n        source_batch.append(source_padded)\n        target_batch.append(target_padded)\n        source_lengths.append(item['source_length'])\n        target_lengths.append(item['target_length'])\n        \n        source_texts.append(item['source'])\n        target_texts.append(item['target'])\n    \n    return {\n        'source': torch.LongTensor(source_batch),\n        'target': torch.LongTensor(target_batch),\n        'source_lengths': torch.LongTensor(source_lengths),\n        'target_lengths': torch.LongTensor(target_lengths),\n        'source_texts': source_texts,\n        'target_texts': target_texts\n    }\n\ndef get_dataloader(file_path, batch_size, shuffle=True):\n    try:\n        dataset = TransliterationDataset(file_path)\n        dataloader = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            collate_fn=collate_fn\n        )\n        return dataloader, dataset\n    except FileNotFoundError as e:\n        print(f\"Error: {str(e)}\")\n        print(f\"Please make sure the Dakshina dataset is downloaded and located correctly.\")\n        print(f\"Expected file path: {file_path}\")\n        print(f\"Current working directory: {os.getcwd()}\")\n        raise\n    except ValueError as e:\n        print(f\"Error: {str(e)}\")\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:27:45.993449Z","iopub.execute_input":"2025-05-20T21:27:45.993709Z","iopub.status.idle":"2025-05-20T21:27:46.009717Z","shell.execute_reply.started":"2025-05-20T21:27:45.993691Z","shell.execute_reply":"2025-05-20T21:27:46.009172Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embed_size, hidden_size, num_layers, dropout, cell_type=\"GRU\"):\n        super(Encoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.embedding = nn.Embedding(input_size, embed_size)\n        self.dropout = nn.Dropout(dropout)\n        \n        if cell_type == \"GRU\":\n            self.rnn = nn.GRU(\n                embed_size,\n                hidden_size,\n                num_layers,\n                dropout=dropout if num_layers > 1 else 0,\n                batch_first=True\n            )\n        elif cell_type == \"LSTM\":\n            self.rnn = nn.LSTM(\n                embed_size,\n                hidden_size,\n                num_layers,\n                dropout=dropout if num_layers > 1 else 0,\n                batch_first=True\n            )\n        else:  # Default to RNN\n            self.rnn = nn.RNN(\n                embed_size,\n                hidden_size,\n                num_layers,\n                dropout=dropout if num_layers > 1 else 0,\n                batch_first=True\n            )\n        \n        self.cell_type = cell_type\n    \n    def forward(self, x, lengths):\n        # x shape: (batch_size, seq_length)\n        embedded = self.dropout(self.embedding(x))\n        \n        # Pack padded batch of sequences for RNN\n        packed = nn.utils.rnn.pack_padded_sequence(\n            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        \n        # Forward propagate RNN\n        outputs, hidden = self.rnn(packed)\n        \n        # Unpack padding\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n        \n        return outputs, hidden\n\n\nclass Decoder(nn.Module):\n    def __init__(self, output_size, embed_size, hidden_size, num_layers, dropout, cell_type=\"GRU\"):\n        super(Decoder, self).__init__()\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        \n        self.embedding = nn.Embedding(output_size, embed_size)\n        self.dropout = nn.Dropout(dropout)\n        \n        if cell_type == \"GRU\":\n            self.rnn = nn.GRU(\n                embed_size,\n                hidden_size,\n                num_layers,\n                dropout=dropout if num_layers > 1 else 0,\n                batch_first=True\n            )\n        elif cell_type == \"LSTM\":\n            self.rnn = nn.LSTM(\n                embed_size,\n                hidden_size,\n                num_layers,\n                dropout=dropout if num_layers > 1 else 0,\n                batch_first=True\n            )\n        else:  # Default to RNN\n            self.rnn = nn.RNN(\n                embed_size,\n                hidden_size,\n                num_layers,\n                dropout=dropout if num_layers > 1 else 0,\n                batch_first=True\n            )\n        \n        self.fc_out = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x, hidden):\n        # x shape: (batch_size, 1)\n        embedded = self.dropout(self.embedding(x))\n        \n        # Forward propagate through RNN\n        output, hidden = self.rnn(embedded, hidden)\n        \n        # Compute output\n        prediction = self.fc_out(output.squeeze(1))\n        \n        return prediction, hidden\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, \n                 input_size, \n                 output_size, \n                 embed_size, \n                 hidden_size, \n                 num_encoder_layers, \n                 num_decoder_layers, \n                 dropout, \n                 cell_type=\"GRU\"):\n        super(Seq2Seq, self).__init__()\n        \n        self.encoder = Encoder(\n            input_size, \n            embed_size, \n            hidden_size, \n            num_encoder_layers, \n            dropout, \n            cell_type\n        )\n        \n        self.decoder = Decoder(\n            output_size, \n            embed_size, \n            hidden_size, \n            num_decoder_layers, \n            dropout, \n            cell_type\n        )\n        \n        self.cell_type = cell_type\n        self.num_encoder_layers = num_encoder_layers\n        self.num_decoder_layers = num_decoder_layers\n        self.hidden_size = hidden_size\n    \n    def forward(self, source, target, source_lengths, teacher_forcing_ratio=0.5):\n        batch_size = source.shape[0]\n        target_len = target.shape[1]\n        target_vocab_size = self.decoder.output_size\n        \n        # Tensor to store decoder outputs\n        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(source.device)\n        \n        # Encode source sequences\n        encoder_outputs, hidden = self.encoder(source, source_lengths)\n        \n        # Process hidden state if encoder and decoder layers differ\n        hidden = self._process_hidden_for_decoder(hidden, batch_size)\n        \n        # First input to the decoder is the < SOS > token\n        decoder_input = target[:, 0].unsqueeze(1)\n        \n        # Decode one character at a time\n        for t in range(1, target_len):\n            output, hidden = self.decoder(decoder_input, hidden)\n            outputs[:, t, :] = output\n            \n            # Teacher forcing: decide whether to use the predicted or actual target as next input\n            teacher_force = random.random() < teacher_forcing_ratio\n            \n            # Get the highest predicted token\n            top1 = output.argmax(1)\n            \n            # If teacher forcing, use actual next token as input; otherwise use predicted token\n            decoder_input = target[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n        \n        return outputs\n    \n    def _process_hidden_for_decoder(self, hidden, batch_size):\n        \"\"\"Process the encoder hidden state for the decoder\"\"\"\n        if self.num_encoder_layers == self.num_decoder_layers:\n            return hidden\n        \n        # For GRU and RNN\n        if self.cell_type != \"LSTM\":\n            if self.num_encoder_layers < self.num_decoder_layers:\n                # Duplicate last layer to match decoder layers\n                additional_layers = self.num_decoder_layers - self.num_encoder_layers\n                last_layer = hidden[-1:].expand(additional_layers, batch_size, self.hidden_size)\n                return torch.cat([hidden, last_layer], dim=0)\n            else:\n                # Use only the last n layers\n                return hidden[-self.num_decoder_layers:]\n        else:\n            # For LSTM (hidden is a tuple of (h_0, c_0))\n            h, c = hidden\n            if self.num_encoder_layers < self.num_decoder_layers:\n                # Duplicate last layer to match decoder layers\n                additional_layers = self.num_decoder_layers - self.num_encoder_layers\n                last_h_layer = h[-1:].expand(additional_layers, batch_size, self.hidden_size)\n                last_c_layer = c[-1:].expand(additional_layers, batch_size, self.hidden_size)\n                new_h = torch.cat([h, last_h_layer], dim=0)\n                new_c = torch.cat([c, last_c_layer], dim=0)\n                return (new_h, new_c)\n            else:\n                # Use only the last n layers\n                return (h[-self.num_decoder_layers:], c[-self.num_decoder_layers:])\n    \n    def predict(self, source, source_len, target_vocab_size, sos_idx, eos_idx, max_len=100):\n        batch_size = source.shape[0]\n        \n        # Encode source sequences\n        encoder_outputs, hidden = self.encoder(source, source_len)\n        \n        # Process hidden state if encoder and decoder layers differ\n        hidden = self._process_hidden_for_decoder(hidden, batch_size)\n        \n        # First input to the decoder is the < SOS > token\n        decoder_input = torch.tensor([[sos_idx]] * batch_size).to(source.device)\n        \n        # Lists to store predicted outputs\n        predictions = []\n        \n        # Flag to indicate if decoding is complete\n        done = [False] * batch_size\n        \n        # Decode one character at a time\n        for _ in range(max_len):\n            output, hidden = self.decoder(decoder_input, hidden)\n            \n            # Get the highest predicted token\n            top1 = output.argmax(1)\n            \n            # Store predicted token\n            predictions.append(top1.unsqueeze(1))\n            \n            # Check if all sequences have reached EOS\n            for i in range(batch_size):\n                if top1[i].item() == eos_idx:\n                    done[i] = True\n            \n            if all(done):\n                break\n            \n            # Use predicted token as next input\n            decoder_input = top1.unsqueeze(1)\n        \n        # Concatenate predictions along the sequence dimension\n        return torch.cat(predictions, dim=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:27:46.010474Z","iopub.execute_input":"2025-05-20T21:27:46.010747Z","iopub.status.idle":"2025-05-20T21:27:46.032515Z","shell.execute_reply.started":"2025-05-20T21:27:46.010725Z","shell.execute_reply":"2025-05-20T21:27:46.031885Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"##### Attention based Seq2seq model using pytorch","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random\n\nclass Attention(nn.Module):\n    def __init__(self, encoder_hidden_size, decoder_hidden_size):\n        super(Attention, self).__init__()\n        self.attn = nn.Linear(encoder_hidden_size + decoder_hidden_size, decoder_hidden_size)\n        self.v = nn.Linear(decoder_hidden_size, 1, bias=False)\n        \n    def forward(self, hidden, encoder_outputs):\n        batch_size = encoder_outputs.shape[0]\n        src_len = encoder_outputs.shape[1]\n        \n        # Reshape hidden to match encoder_outputs for concatenation\n        # For GRU/RNN: hidden is [num_layers, batch_size, hidden_size]\n        # For LSTM: hidden is a tuple of [h, c], each [num_layers, batch_size, hidden_size]\n        \n        # Handle both LSTM and GRU/RNN cases\n        if isinstance(hidden, tuple):  # LSTM case\n            hidden_state = hidden[0]  # Use the hidden state (h), not cell state (c)\n        else:  # GRU/RNN case\n            hidden_state = hidden\n            \n        # Take the last layer's hidden state and expand it to match encoder_outputs length\n        last_hidden = hidden_state[-1].unsqueeze(1).expand(-1, src_len, -1)\n        \n        # Now hidden and encoder_outputs should have compatible dimensions for concatenation\n        # last_hidden: [batch_size, src_len, hidden_size]\n        # encoder_outputs: [batch_size, src_len, hidden_size]\n        energy = torch.tanh(self.attn(torch.cat((last_hidden, encoder_outputs), dim=2)))\n        attention = self.v(energy).squeeze(2)\n        \n        return F.softmax(attention, dim=1)\n\nclass AttentionDecoder(nn.Module):\n    def __init__(self, output_size, embed_size, hidden_size, num_layers, dropout, cell_type=\"GRU\"):\n        super(AttentionDecoder, self).__init__()\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        \n        self.embedding = nn.Embedding(output_size, embed_size)\n        self.dropout = nn.Dropout(dropout)\n        self.attention = Attention(hidden_size, hidden_size)\n        \n        if cell_type == \"GRU\":\n            self.rnn = nn.GRU(\n                embed_size + hidden_size,\n                hidden_size,\n                num_layers,\n                dropout=dropout if num_layers > 1 else 0,\n                batch_first=True\n            )\n        elif cell_type == \"LSTM\":\n            self.rnn = nn.LSTM(\n                embed_size + hidden_size,\n                hidden_size,\n                num_layers,\n                dropout=dropout if num_layers > 1 else 0,\n                batch_first=True\n            )\n        else:\n            self.rnn = nn.RNN(\n                embed_size + hidden_size,\n                hidden_size,\n                num_layers,\n                dropout=dropout if num_layers > 1 else 0,\n                batch_first=True\n            )\n        \n        self.fc_out = nn.Linear(hidden_size * 2 + embed_size, output_size)\n    \n    def forward(self, x, hidden, encoder_outputs):\n        x = x.unsqueeze(1)\n        embedded = self.dropout(self.embedding(x))\n        \n        a = self.attention(hidden, encoder_outputs)\n        a = a.unsqueeze(1)\n        \n        weighted = torch.bmm(a, encoder_outputs)\n        \n        rnn_input = torch.cat((embedded, weighted), dim=2)\n        \n        output, hidden = self.rnn(rnn_input, hidden)\n        \n        output = output.squeeze(1)\n        weighted = weighted.squeeze(1)\n        embedded = embedded.squeeze(1)\n        \n        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n        \n        return prediction, hidden, a.squeeze(1)\n\nclass Seq2SeqAttention(nn.Module):\n    def __init__(self, \n                 input_size, \n                 output_size, \n                 embed_size, \n                 hidden_size, \n                 num_encoder_layers, \n                 num_decoder_layers, \n                 dropout, \n                 cell_type=\"GRU\"):\n        super(Seq2SeqAttention, self).__init__()\n        \n        #from seq2seq import Encoder\n        \n        self.encoder = Encoder(\n            input_size, \n            embed_size, \n            hidden_size, \n            num_encoder_layers, \n            dropout, \n            cell_type\n        )\n        \n        self.decoder = AttentionDecoder(\n            output_size, \n            embed_size, \n            hidden_size, \n            num_decoder_layers, \n            dropout, \n            cell_type\n        )\n        \n        self.cell_type = cell_type\n        self.num_encoder_layers = num_encoder_layers\n        self.num_decoder_layers = num_decoder_layers\n        self.hidden_size = hidden_size\n    \n    def forward(self, source, target, source_lengths, teacher_forcing_ratio=0.5):\n        batch_size = source.shape[0]\n        target_len = target.shape[1]\n        target_vocab_size = self.decoder.output_size\n        \n        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(source.device)\n        attentions = torch.zeros(batch_size, target_len, source.shape[1]).to(source.device)\n        \n        encoder_outputs, hidden = self.encoder(source, source_lengths)\n        \n        hidden = self._process_hidden_for_decoder(hidden, batch_size)\n        \n        decoder_input = target[:, 0]\n        \n        for t in range(1, target_len):\n            output, hidden, attention = self.decoder(decoder_input, hidden, encoder_outputs)\n            outputs[:, t, :] = output\n            attentions[:, t, :] = attention\n            \n            teacher_force = random.random() < teacher_forcing_ratio\n            \n            top1 = output.argmax(1)\n            \n            decoder_input = target[:, t] if teacher_force else top1\n        \n        return outputs, attentions\n    \n    def _process_hidden_for_decoder(self, hidden, batch_size):\n        if self.num_encoder_layers == self.num_decoder_layers:\n            return hidden\n        \n        if self.cell_type != \"LSTM\":\n            if self.num_encoder_layers < self.num_decoder_layers:\n                additional_layers = self.num_decoder_layers - self.num_encoder_layers\n                last_layer = hidden[-1:].expand(additional_layers, batch_size, self.hidden_size)\n                return torch.cat([hidden, last_layer], dim=0)\n            else:\n                return hidden[-self.num_decoder_layers:]\n        else:\n            h, c = hidden\n            if self.num_encoder_layers < self.num_decoder_layers:\n                additional_layers = self.num_decoder_layers - self.num_encoder_layers\n                last_h_layer = h[-1:].expand(additional_layers, batch_size, self.hidden_size)\n                last_c_layer = c[-1:].expand(additional_layers, batch_size, self.hidden_size)\n                new_h = torch.cat([h, last_h_layer], dim=0)\n                new_c = torch.cat([c, last_c_layer], dim=0)\n                return (new_h, new_c)\n            else:\n                return (h[-self.num_decoder_layers:], c[-self.num_decoder_layers:])\n    \n    def predict(self, source, source_len, target_vocab_size, sos_idx, eos_idx, max_len=100):\n        batch_size = source.shape[0]\n        \n        encoder_outputs, hidden = self.encoder(source, source_len)\n        \n        hidden = self._process_hidden_for_decoder(hidden, batch_size)\n        \n        decoder_input = torch.tensor([sos_idx] * batch_size).to(source.device)\n        \n        predictions = []\n        attention_weights = []\n        \n        done = [False] * batch_size\n        \n        for _ in range(max_len):\n            output, hidden, attention = self.decoder(decoder_input, hidden, encoder_outputs)\n            \n            top1 = output.argmax(1)\n            \n            predictions.append(top1.unsqueeze(1))\n            attention_weights.append(attention.unsqueeze(1))\n            \n            for i in range(batch_size):\n                if top1[i].item() == eos_idx:\n                    done[i] = True\n            \n            if all(done):\n                break\n            \n            decoder_input = top1\n        \n        return torch.cat(predictions, dim=1), torch.cat(attention_weights, dim=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:27:46.033211Z","iopub.execute_input":"2025-05-20T21:27:46.033481Z","iopub.status.idle":"2025-05-20T21:27:46.055906Z","shell.execute_reply.started":"2025-05-20T21:27:46.033461Z","shell.execute_reply":"2025-05-20T21:27:46.055234Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"##### utilities function for calculating accuracy and checkpoints","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\nimport numpy as np\n\ndef calculate_accuracy(predictions, targets, target_lengths, ignore_index=0):\n    \"\"\"\n    Calculate the accuracy of the predictions against the targets.\n    A prediction is considered correct only if all characters in the sequence match.\n    \"\"\"\n    batch_size = predictions.size(0)\n    correct = 0\n    \n    for i in range(batch_size):\n        pred_seq = predictions[i].cpu().numpy()\n        target_seq = targets[i, 1:target_lengths[i]-1].cpu().numpy()  # Exclude SOS and EOS tokens\n        \n        # Check if prediction exactly matches target\n        if len(pred_seq) == len(target_seq) and np.array_equal(pred_seq, target_seq):\n            correct += 1\n    \n    return correct / batch_size\n\ndef save_checkpoint(model, optimizer, epoch, accuracy, filename):\n    \"\"\"Save model checkpoint\"\"\"\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'accuracy': accuracy\n    }, filename)\n\ndef load_checkpoint(model, optimizer, filename):\n    \"\"\"Load model checkpoint\"\"\"\n    checkpoint = torch.load(filename)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    accuracy = checkpoint.get('accuracy', 0.0)\n    return model, optimizer, epoch, accuracy\n\ndef indices_to_string(indices, idx_to_char, eos_idx=None):\n    \"\"\"Convert a sequence of indices to a string\"\"\"\n    if eos_idx is not None:\n        # Find the index of the first EOS token\n        try:\n            eos_pos = indices.index(eos_idx)\n            indices = indices[:eos_pos]\n        except ValueError:\n            pass  # No EOS token found\n    \n    return ''.join([idx_to_char[idx] for idx in indices if idx in idx_to_char and idx_to_char[idx] not in ['<PAD>', '< SOS >', '<EOS>']])\n\ndef create_directory(directory):\n    \"\"\"Create directory if it doesn't exist\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:27:46.056645Z","iopub.execute_input":"2025-05-20T21:27:46.056959Z","iopub.status.idle":"2025-05-20T21:27:46.076359Z","shell.execute_reply.started":"2025-05-20T21:27:46.056929Z","shell.execute_reply":"2025-05-20T21:27:46.075718Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"Config contains (Data parameter,Model Parameter and Training Parameter)","metadata":{}},{"cell_type":"code","source":"import os\n\nclass Config:\n    # Data parameters\n    language = os.environ.get(\"TRANSLITERATION_LANGUAGE\", \"hi\")  # Get language from env var or default to \"hi\"\n    #base_dir = os.path.dirname(os.path.abspath(__file__))  # Get the directory of the current file\n    #data_dir = os.path.join(base_dir, \"dakshina_dataset_v1.0\", f\"{language}\", \"lexicons\") \n    input_dir = \"/kaggle/input/dataset\"  # Read-only input path\n    base_dir = \"/kaggle/working\"         # Writable path for outputs\n    \n    data_dir = os.path.join(input_dir, \"dakshina_dataset_v1.0\", language, \"lexicons\")\n    # Check if data directory exists, if not, try to find it elsewhere\n    if not os.path.exists(data_dir):\n        # Try to find the dakshina dataset in parent directories\n        parent_dir = os.path.dirname(base_dir)\n        alt_data_dir = os.path.join(parent_dir, \"dakshina_dataset_v1.0\", f\"{language}\", \"lexicons\")\n        if os.path.exists(alt_data_dir):\n            data_dir = alt_data_dir\n    \n    train_file = os.path.join(data_dir, f\"{language}.translit.sampled.train.tsv\")\n    val_file = os.path.join(data_dir, f\"{language}.translit.sampled.dev.tsv\")\n    test_file = os.path.join(data_dir, f\"{language}.translit.sampled.test.tsv\")\n    \n    # Model parameters (default values)\n    embed_size = 64\n    hidden_size = 128\n    num_encoder_layers = 1\n    num_decoder_layers = 1\n    dropout = 0.2\n    cell_type = \"GRU\"  # Options: RNN, LSTM, GRU\n    \n    # Training parameters\n    batch_size = 64\n    epochs = 20\n    learning_rate = 0.001\n    teacher_forcing_ratio = 0.5\n    \n    # Decoding parameters\n    beam_size = 1  # 1 for greedy decoding\n    \n    # W&B parameters\n    wandb_project = \"DL_A3_seq2seq\"\n    wandb_entity = \"da24m006-iit-madras\"  \n    \n    # Save directories\n    model_dir = os.path.join(base_dir, \"saved_models\", language)\n    # Create directory for saving model if it doesn't exist\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir, exist_ok=True)\n    \n    prediction_dir = os.path.join(base_dir, \"predictions\", language)\n    # Create directory for saving prediction if it doesn't exist\n    if not os.path.exists(prediction_dir):\n        os.makedirs(prediction_dir, exist_ok=True)\n\n    \n    def __init__(self):\n        # Verify paths and print info\n        print(f\"Base directory: {self.base_dir}\")\n        print(f\"Language: {self.language}\")\n        print(f\"Data directory: {self.data_dir}\")\n        print(f\"Train file exists: {os.path.exists(self.train_file)}\")\n        print(f\"Val file exists: {os.path.exists(self.val_file)}\")\n        print(f\"Test file exists: {os.path.exists(self.test_file)}\")\n        \n        # Print sample data from files if they exist\n        self._print_sample_data(self.train_file, \"train\")\n        self._print_sample_data(self.val_file, \"validation\")\n        self._print_sample_data(self.test_file, \"test\")\n    \n    def _print_sample_data(self, file_path, name):\n        \"\"\"Print a sample of data from the file for debugging\"\"\"\n        if os.path.exists(file_path):\n            try:\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    lines = f.readlines()[:3]  # Get first 3 lines\n                    if lines:\n                        print(f\"\\nSample {name} data:\")\n                        for line in lines:\n                            print(f\"  {line.strip()}\")\n                        print(f\"  ... (total lines: {sum(1 for _ in open(file_path, 'r', encoding='utf-8'))})\")\n                    else:\n                        print(f\"\\nWarning: {name} file exists but is empty: {file_path}\")\n            except Exception as e:\n                print(f\"\\nError reading {name} file: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:27:46.078627Z","iopub.execute_input":"2025-05-20T21:27:46.079064Z","iopub.status.idle":"2025-05-20T21:27:46.098831Z","shell.execute_reply.started":"2025-05-20T21:27:46.079048Z","shell.execute_reply":"2025-05-20T21:27:46.098211Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def train_attention(config, run_name=None, init_wandb=True):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Only initialize wandb if specified\n    if init_wandb:\n        if run_name:\n            wandb.init(project=config.wandb_project, entity=config.wandb_entity, name=run_name, config=vars(config))\n        else:\n            default_run_name = f\"attn_{config.cell_type}_{config.language}_e{config.embed_size}_h{config.hidden_size}\"\n            wandb.init(project=config.wandb_project, entity=config.wandb_entity, name=default_run_name, config=vars(config))\n        \n        wandb.config.update({\"language\": config.language})\n    \n    print(f\"\\nTraining attention-based transliteration model for language: {config.language}\")\n    print(f\"Using dataset from: {config.data_dir}\\n\")\n    \n    train_loader, train_dataset = get_dataloader(\n        config.train_file, \n        config.batch_size, \n        shuffle=True\n    )\n    \n    val_loader, val_dataset = get_dataloader(\n        config.val_file, \n        config.batch_size, \n        shuffle=False\n    )\n    \n    model = Seq2SeqAttention(\n        input_size=train_dataset.source_vocab_size,\n        output_size=train_dataset.target_vocab_size,\n        embed_size=config.embed_size,\n        hidden_size=config.hidden_size,\n        num_encoder_layers=config.num_encoder_layers,\n        num_decoder_layers=config.num_decoder_layers,\n        dropout=config.dropout,\n        cell_type=config.cell_type\n    )\n    \n    model = model.to(device)\n    \n    criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n    \n    create_directory(config.model_dir)\n    \n    best_accuracy = 0.0\n    \n    for epoch in range(config.epochs):\n        model.train()\n        total_loss = 0\n        \n        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config.epochs}')\n        \n        for batch_idx, batch in enumerate(pbar):\n            source = batch['source'].to(device)\n            target = batch['target'].to(device)\n            source_lengths = batch['source_lengths']\n            \n            optimizer.zero_grad()\n            \n            output, _ = model(source, target, source_lengths, config.teacher_forcing_ratio)\n            \n            output_dim = output.shape[-1]\n            output = output[:, 1:].reshape(-1, output_dim)\n            target = target[:, 1:].reshape(-1)\n            \n            loss = criterion(output, target)\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            total_loss += loss.item()\n            pbar.set_postfix({'loss': loss.item()})\n        \n        avg_loss = total_loss / len(train_loader)\n        \n        model.eval()\n        val_loss, val_accuracy = evaluate_attention(model, val_loader, criterion, device, train_dataset)\n        \n        wandb.log({\n            'epoch': epoch + 1,\n            'train_loss': avg_loss,\n            'val_loss': val_loss,\n            'val_accuracy': val_accuracy,\n            'language': config.language\n        })\n        \n        print(f'Epoch: {epoch+1}, Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n        \n        if val_accuracy > best_accuracy:\n            best_accuracy = val_accuracy\n            if not os.path.exists(os.path.join(config.model_dir)):\n                os.makedirs(os.path.join(config.model_dir))\n\n            model_path = os.path.join(config.model_dir, f'best_attention_model_{config.language}_{wandb.run.id}.pt')\n            save_checkpoint(\n                model, \n                optimizer, \n                epoch, \n                val_accuracy, \n                model_path\n            )\n            \n            try:\n                wandb_model_path = os.path.join(wandb.run.dir, \"best_attention_model.pt\")\n                torch.save(model.state_dict(), wandb_model_path)\n                \n                artifact = wandb.Artifact(f\"attention-model-{config.language}-{wandb.run.id}\", type=\"model\")\n                artifact.add_file(wandb_model_path)\n                wandb.log_artifact(artifact)\n                \n                print(f\"Model successfully saved to W&B as artifact\")\n            except Exception as e:\n                print(f\"Failed to save model to W&B: {str(e)}\")\n                print(f\"Model was saved locally to {model_path}\")\n        \n        if (epoch + 1) % 5 == 0:\n            checkpoint_path = os.path.join(config.model_dir, f'checkpoint_attention_{config.language}_{epoch+1}_{wandb.run.id}.pt')\n            save_checkpoint(\n                model, \n                optimizer, \n                epoch, \n                val_accuracy, \n                checkpoint_path\n            )\n    \n    # Only finish wandb if we initialized it\n    if init_wandb:\n        wandb.finish()\n        \n    return model, best_accuracy\n\ndef evaluate_attention(model, dataloader, criterion, device, dataset):\n    model.eval()\n    total_loss = 0\n    total_correct = 0\n    total_examples = 0\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            source = batch['source'].to(device)\n            target = batch['target'].to(device)\n            source_lengths = batch['source_lengths']\n            \n            output, _ = model(source, target, source_lengths, teacher_forcing_ratio=0)\n            \n            output_dim = output.shape[-1]\n            output_for_loss = output[:, 1:].reshape(-1, output_dim)\n            target_for_loss = target[:, 1:].reshape(-1)\n            \n            loss = criterion(output_for_loss, target_for_loss)\n            total_loss += loss.item()\n            \n            predictions, _ = model.predict(\n                source,\n                source_lengths,\n                dataset.target_vocab_size,\n                dataset.target_char_to_idx['< SOS >'],\n                dataset.target_char_to_idx['<EOS>']\n            )\n            \n            for i in range(len(predictions)):\n                pred_seq = [idx.item() for idx in predictions[i]]\n                pred_str = indices_to_string(pred_seq, dataset.target_idx_to_char, dataset.target_char_to_idx['<EOS>'])\n                target_str = batch['target_texts'][i]\n                \n                if pred_str == target_str:\n                    total_correct += 1\n            \n            total_examples += len(predictions)\n    \n    avg_loss = total_loss / len(dataloader)\n    accuracy = total_correct / total_examples\n    \n    return avg_loss, accuracy\n\n# if __name__ == \"__main__\":\n#     config = Config()\n#     train_attention(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:27:46.099767Z","iopub.execute_input":"2025-05-20T21:27:46.100012Z","iopub.status.idle":"2025-05-20T21:27:46.120091Z","shell.execute_reply.started":"2025-05-20T21:27:46.099953Z","shell.execute_reply":"2025-05-20T21:27:46.119570Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"sweep_attention_configuration = {\n    'method': 'bayes',\n    'metric': {\n        'name': 'val_accuracy',\n        'goal': 'maximize'\n    },\n    'parameters': {\n        'embed_size': {\n            'values': [32, 64, 128, 256]\n        },\n        'hidden_size': {\n            'values': [64, 128, 256, 512]\n        },\n        'num_encoder_layers': {\n            'values': [1, 2]\n        },\n        'num_decoder_layers': {\n            'values': [1, 2]\n        },\n        'cell_type': {\n            'values': ['GRU', 'LSTM']\n        },\n        'dropout': {\n            'values': [0.2, 0.3, 0.5]\n        },\n        'learning_rate': {\n            'values': [0.0001, 0.001, 0.002]\n        },\n        'batch_size': {\n            'values': [64, 128]\n        },\n        'teacher_forcing_ratio': {\n            'values': [0.5, 0.7, 0.9]\n        }\n    }\n}\n\ndef sweep_train_attention(config_defaults=None):\n    config = Config()  # initialize config early to construct run_name\n\n    # Construct initial run name from default values\n    run_name = f\"attn_{config.language}_{config.cell_type}_drop{config.dropout}_es{config.embed_size}_hs{config.hidden_size}_en{config.num_encoder_layers}_de{config.num_decoder_layers}_lr{config.learning_rate}\"\n    \n    with wandb.init(config=config_defaults, name=run_name) as run:\n        wandb_config = wandb.config\n        for key, value in wandb_config.items():\n            if hasattr(config, key):\n                setattr(config, key, value)\n\n        # Rebuild run name with updated sweep values\n        run_name = f\"attn_{config.language}_{config.cell_type}_drop{config.dropout}_es{config.embed_size}_hs{config.hidden_size}_en{config.num_encoder_layers}_de{config.num_decoder_layers}_lr{config.learning_rate}\"\n        wandb.run.name = run_name\n        wandb.run.save()\n\n        # Train the model — avoid reinitializing wandb\n        model, accuracy = train_attention(config, run_name=run_name, init_wandb=False)\n\n        # Log final accuracy\n        run.summary['best_accuracy'] = accuracy\n\n# def sweep_train_attention(config_defaults=None):\n#     # The issue is here - we need to ensure wandb.init() is called before using wandb.summary\n#     run = wandb.init(config=config_defaults)\n    \n#     config = Config()\n    \n#     # Update config with parameters from sweep\n#     wandb_config = wandb.config\n#     for key, value in wandb_config.items():\n#         if hasattr(config, key):\n#             setattr(config, key, value)\n    \n#     # Create model name based on parameters\n#     run_name = f\"attn_{config.language}_{config.cell_type}_e{config.embed_size}_h{config.hidden_size}_en{config.num_encoder_layers}_de{config.num_decoder_layers}_d{config.dropout}_lr{config.learning_rate}\"\n            \n#     # Train the model\n#     # Don't initialize wandb again in train_attention\n#     model, accuracy = train_attention(config, run_name=run_name, init_wandb=False)\n    \n#     # Log the final result\n#     wandb.run.summary['best_accuracy'] = accuracy\n#     # No need to call wandb.finish() as the context manager will handle it\n\n\n# def main():\n#     parser = argparse.ArgumentParser(description='Run W&B sweep for attention transliteration model')\n#     parser.add_argument('--sweep_id', type=str, help='The sweep ID to use (if already created)')\n#     parser.add_argument('--count', type=int, default=20, help='Number of sweep runs')\n#     parser.add_argument('--language', type=str, default='hi', \n#                         choices=['bn', 'gu', 'hi', 'kn', 'ml', 'mr', 'pa', 'sd', 'si', 'ta', 'te', 'ur'],\n#                         help='Language code to use (default: hi)')\n#     args = parser.parse_args()\n    \n#     # Set language in environment variable\n#     os.environ[\"TRANSLITERATION_LANGUAGE\"] = args.language\n    \n#     # Set up W&B project\n#     config = Config()\n    \n#     if args.sweep_id:\n#         sweep_id = args.sweep_id\n#     else:\n#         sweep_id = wandb.sweep(sweep=sweep_attention_configuration, project=config.wandb_project, entity=config.wandb_entity)\n    \n#     # Start sweep agent\n#     wandb.agent(sweep_id, function=sweep_train_attention, count=args.count)\n\n# if __name__ == \"__main__\":\n#     main()\n\nos.environ[\"TRANSLITERATION_LANGUAGE\"] = \"hi\"\nconfig = Config()\n\nsweep_id = wandb.sweep(sweep=sweep_attention_configuration, project=config.wandb_project, entity=config.wandb_entity)\nwandb.agent(sweep_id, function=sweep_train_attention, count=30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T21:28:25.469986Z","iopub.execute_input":"2025-05-20T21:28:25.470694Z","iopub.status.idle":"2025-05-20T21:46:07.516092Z","shell.execute_reply.started":"2025-05-20T21:28:25.470669Z","shell.execute_reply":"2025-05-20T21:46:07.515272Z"}},"outputs":[{"name":"stdout","text":"Base directory: /kaggle/working\nLanguage: hi\nData directory: /kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons\nTrain file exists: True\nVal file exists: True\nTest file exists: True\n\nSample train data:\n  अं\tan\t3\n  अंकगणित\tankganit\t3\n  अंकल\tuncle\t4\n  ... (total lines: 44204)\n\nSample validation data:\n  अंकन\tankan\t3\n  अंगकोर\tangkor\t3\n  अंगिरा\tangira\t3\n  ... (total lines: 4358)\n\nSample test data:\n  अंक\tank\t5\n  अंक\tanka\t1\n  अंकित\tankit\t3\n  ... (total lines: 4502)\nCreate sweep with ID: ij9kejdj\nSweep URL: https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/ij9kejdj\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yx3v0jfb with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.002\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decoder_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encoder_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.7\n","output_type":"stream"},{"name":"stdout","text":"Base directory: /kaggle/working\nLanguage: hi\nData directory: /kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons\nTrain file exists: True\nVal file exists: True\nTest file exists: True\n\nSample train data:\n  अं\tan\t3\n  अंकगणित\tankganit\t3\n  अंकल\tuncle\t4\n  ... (total lines: 44204)\n\nSample validation data:\n  अंकन\tankan\t3\n  अंगकोर\tangkor\t3\n  अंगिरा\tangira\t3\n  ... (total lines: 4358)\n\nSample test data:\n  अंक\tank\t5\n  अंक\tanka\t1\n  अंकित\tankit\t3\n  ... (total lines: 4502)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_212833-yx3v0jfb</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/yx3v0jfb' target=\"_blank\">attn_hi_GRU_drop0.2_es64_hs128_en1_de1_lr0.001</a></strong> to <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/ij9kejdj' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/ij9kejdj</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/ij9kejdj' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/ij9kejdj</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/yx3v0jfb' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/yx3v0jfb</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n","output_type":"stream"},{"name":"stdout","text":"\nTraining attention-based transliteration model for language: hi\nUsing dataset from: /kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/20: 100%|██████████| 691/691 [00:27<00:00, 25.23it/s, loss=1.15] \n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Train Loss: 1.4878, Val Loss: 7.8537, Val Accuracy: 0.2129\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 100%|██████████| 691/691 [00:26<00:00, 25.87it/s, loss=0.717]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Train Loss: 0.8968, Val Loss: 8.5219, Val Accuracy: 0.2705\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 100%|██████████| 691/691 [00:26<00:00, 25.80it/s, loss=0.846]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, Train Loss: 0.7952, Val Loss: 8.7736, Val Accuracy: 0.2935\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 100%|██████████| 691/691 [00:26<00:00, 25.65it/s, loss=0.597]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, Train Loss: 0.7379, Val Loss: 8.9442, Val Accuracy: 0.3077\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 100%|██████████| 691/691 [00:26<00:00, 25.63it/s, loss=0.951]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 5, Train Loss: 0.7013, Val Loss: 9.2920, Val Accuracy: 0.3192\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/20: 100%|██████████| 691/691 [00:27<00:00, 25.53it/s, loss=0.657]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 6, Train Loss: 0.6627, Val Loss: 9.4825, Val Accuracy: 0.3219\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/20: 100%|██████████| 691/691 [00:27<00:00, 25.51it/s, loss=0.559]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 7, Train Loss: 0.6365, Val Loss: 9.7847, Val Accuracy: 0.3350\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/20: 100%|██████████| 691/691 [00:27<00:00, 25.43it/s, loss=0.741]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 8, Train Loss: 0.6197, Val Loss: 9.5689, Val Accuracy: 0.3401\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/20: 100%|██████████| 691/691 [00:27<00:00, 25.36it/s, loss=0.531]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 9, Train Loss: 0.6073, Val Loss: 9.8540, Val Accuracy: 0.3460\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/20: 100%|██████████| 691/691 [00:27<00:00, 25.27it/s, loss=0.56] \n","output_type":"stream"},{"name":"stdout","text":"Epoch: 10, Train Loss: 0.6000, Val Loss: 9.9204, Val Accuracy: 0.3479\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/20: 100%|██████████| 691/691 [00:27<00:00, 25.44it/s, loss=1.03] \n","output_type":"stream"},{"name":"stdout","text":"Epoch: 11, Train Loss: 0.5857, Val Loss: 9.9005, Val Accuracy: 0.3612\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/20: 100%|██████████| 691/691 [00:27<00:00, 25.50it/s, loss=0.407]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 12, Train Loss: 0.5790, Val Loss: 10.0336, Val Accuracy: 0.3637\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/20: 100%|██████████| 691/691 [00:26<00:00, 25.63it/s, loss=0.845]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 13, Train Loss: 0.5713, Val Loss: 10.0400, Val Accuracy: 0.3706\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/20: 100%|██████████| 691/691 [00:27<00:00, 25.56it/s, loss=0.503]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 14, Train Loss: 0.5628, Val Loss: 10.1634, Val Accuracy: 0.3644\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/20: 100%|██████████| 691/691 [00:27<00:00, 25.31it/s, loss=0.394]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 15, Train Loss: 0.5545, Val Loss: 10.2600, Val Accuracy: 0.3612\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/20: 100%|██████████| 691/691 [00:27<00:00, 25.49it/s, loss=0.594]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 16, Train Loss: 0.5428, Val Loss: 10.2707, Val Accuracy: 0.3616\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/20: 100%|██████████| 691/691 [00:28<00:00, 24.54it/s, loss=0.545]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 17, Train Loss: 0.5381, Val Loss: 10.1949, Val Accuracy: 0.3676\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/20: 100%|██████████| 691/691 [00:27<00:00, 24.80it/s, loss=0.385]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 18, Train Loss: 0.5353, Val Loss: 10.3709, Val Accuracy: 0.3759\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/20: 100%|██████████| 691/691 [00:27<00:00, 24.94it/s, loss=0.767]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 19, Train Loss: 0.5294, Val Loss: 10.3428, Val Accuracy: 0.3749\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/20: 100%|██████████| 691/691 [00:27<00:00, 25.04it/s, loss=0.381]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 20, Train Loss: 0.5249, Val Loss: 10.4838, Val Accuracy: 0.3754\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▅▆▆▆▆▇▇▇▇██▇▇████</td></tr><tr><td>val_loss</td><td>▁▃▃▄▅▅▆▆▆▇▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_accuracy</td><td>0.37586</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>language</td><td>hi</td></tr><tr><td>train_loss</td><td>0.52493</td></tr><tr><td>val_accuracy</td><td>0.3754</td></tr><tr><td>val_loss</td><td>10.48379</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">attn_hi_GRU_drop0.3_es64_hs64_en1_de2_lr0.002</strong> at: <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/yx3v0jfb' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/yx3v0jfb</a><br> View project at: <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq</a><br>Synced 5 W&B file(s), 0 media file(s), 28 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_212833-yx3v0jfb/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g3ldyd8j with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decoder_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encoder_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.7\n","output_type":"stream"},{"name":"stdout","text":"Base directory: /kaggle/working\nLanguage: hi\nData directory: /kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons\nTrain file exists: True\nVal file exists: True\nTest file exists: True\n\nSample train data:\n  अं\tan\t3\n  अंकगणित\tankganit\t3\n  अंकल\tuncle\t4\n  ... (total lines: 44204)\n\nSample validation data:\n  अंकन\tankan\t3\n  अंगकोर\tangkor\t3\n  अंगिरा\tangira\t3\n  ... (total lines: 4358)\n\nSample test data:\n  अंक\tank\t5\n  अंक\tanka\t1\n  अंकित\tankit\t3\n  ... (total lines: 4502)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_213850-g3ldyd8j</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/g3ldyd8j' target=\"_blank\">attn_hi_GRU_drop0.2_es64_hs128_en1_de1_lr0.001</a></strong> to <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/ij9kejdj' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/ij9kejdj</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/ij9kejdj' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/ij9kejdj</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/g3ldyd8j' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/g3ldyd8j</a>"},"metadata":{}},{"name":"stdout","text":"\nTraining attention-based transliteration model for language: hi\nUsing dataset from: /kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/20: 100%|██████████| 346/346 [00:15<00:00, 22.71it/s, loss=1.43]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Train Loss: 2.3965, Val Loss: 6.1208, Val Accuracy: 0.0961\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 100%|██████████| 346/346 [00:15<00:00, 22.72it/s, loss=0.984]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Train Loss: 1.1671, Val Loss: 7.0064, Val Accuracy: 0.1893\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 100%|██████████| 346/346 [00:15<00:00, 22.86it/s, loss=0.881]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, Train Loss: 0.9477, Val Loss: 7.5208, Val Accuracy: 0.2196\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 100%|██████████| 346/346 [00:15<00:00, 22.37it/s, loss=0.614]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, Train Loss: 0.8625, Val Loss: 7.8779, Val Accuracy: 0.2533\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 100%|██████████| 346/346 [00:15<00:00, 22.90it/s, loss=0.898]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 5, Train Loss: 0.7737, Val Loss: 8.2675, Val Accuracy: 0.2744\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/20: 100%|██████████| 346/346 [00:15<00:00, 22.70it/s, loss=0.623]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 6, Train Loss: 0.7180, Val Loss: 8.4963, Val Accuracy: 0.2990\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/20: 100%|██████████| 346/346 [00:15<00:00, 22.85it/s, loss=0.753]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 7, Train Loss: 0.6777, Val Loss: 8.5474, Val Accuracy: 0.2910\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/20: 100%|██████████| 346/346 [00:15<00:00, 22.83it/s, loss=0.518]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 8, Train Loss: 0.6370, Val Loss: 8.7996, Val Accuracy: 0.3173\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/20: 100%|██████████| 346/346 [00:15<00:00, 22.60it/s, loss=0.807]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 9, Train Loss: 0.6078, Val Loss: 8.8821, Val Accuracy: 0.3295\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/20: 100%|██████████| 346/346 [00:15<00:00, 22.81it/s, loss=0.435]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 10, Train Loss: 0.5818, Val Loss: 9.0951, Val Accuracy: 0.3371\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/20: 100%|██████████| 346/346 [00:15<00:00, 22.68it/s, loss=0.394]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 11, Train Loss: 0.5579, Val Loss: 9.2304, Val Accuracy: 0.3648\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/20: 100%|██████████| 346/346 [00:15<00:00, 22.70it/s, loss=0.572]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 12, Train Loss: 0.5298, Val Loss: 9.3852, Val Accuracy: 0.3720\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/20: 100%|██████████| 346/346 [00:15<00:00, 22.60it/s, loss=0.515]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 13, Train Loss: 0.5101, Val Loss: 9.3997, Val Accuracy: 0.3777\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/20: 100%|██████████| 346/346 [00:15<00:00, 22.87it/s, loss=0.412]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 14, Train Loss: 0.4867, Val Loss: 9.5317, Val Accuracy: 0.3871\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/20: 100%|██████████| 346/346 [00:15<00:00, 22.74it/s, loss=0.557]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 15, Train Loss: 0.4702, Val Loss: 9.6370, Val Accuracy: 0.3814\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/20: 100%|██████████| 346/346 [00:15<00:00, 22.71it/s, loss=0.392]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 16, Train Loss: 0.4575, Val Loss: 9.7868, Val Accuracy: 0.3855\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/20: 100%|██████████| 346/346 [00:15<00:00, 22.82it/s, loss=0.454]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 17, Train Loss: 0.4433, Val Loss: 9.8602, Val Accuracy: 0.3894\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/20: 100%|██████████| 346/346 [00:15<00:00, 22.67it/s, loss=0.245]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 18, Train Loss: 0.4303, Val Loss: 9.9029, Val Accuracy: 0.4002\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/20: 100%|██████████| 346/346 [00:15<00:00, 22.78it/s, loss=0.362]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 19, Train Loss: 0.4112, Val Loss: 10.0010, Val Accuracy: 0.4052\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/20: 100%|██████████| 346/346 [00:15<00:00, 22.59it/s, loss=0.456]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 20, Train Loss: 0.4018, Val Loss: 10.0339, Val Accuracy: 0.4009\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▅▅▆▅▆▆▆▇▇▇█▇█████</td></tr><tr><td>val_loss</td><td>▁▃▄▄▅▅▅▆▆▆▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_accuracy</td><td>0.40523</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>language</td><td>hi</td></tr><tr><td>train_loss</td><td>0.40183</td></tr><tr><td>val_accuracy</td><td>0.40087</td></tr><tr><td>val_loss</td><td>10.03393</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">attn_hi_GRU_drop0.3_es128_hs512_en1_de1_lr0.0001</strong> at: <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/g3ldyd8j' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/g3ldyd8j</a><br> View project at: <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq</a><br>Synced 5 W&B file(s), 0 media file(s), 32 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_213850-g3ldyd8j/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: js51yip4 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decoder_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encoder_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.9\n","output_type":"stream"},{"name":"stdout","text":"Base directory: /kaggle/working\nLanguage: hi\nData directory: /kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons\nTrain file exists: True\nVal file exists: True\nTest file exists: True\n\nSample train data:\n  अं\tan\t3\n  अंकगणित\tankganit\t3\n  अंकल\tuncle\t4\n  ... (total lines: 44204)\n\nSample validation data:\n  अंकन\tankan\t3\n  अंगकोर\tangkor\t3\n  अंगिरा\tangira\t3\n  ... (total lines: 4358)\n\nSample test data:\n  अंक\tank\t5\n  अंक\tanka\t1\n  अंकित\tankit\t3\n  ... (total lines: 4502)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_214504-js51yip4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/js51yip4' target=\"_blank\">attn_hi_GRU_drop0.2_es64_hs128_en1_de1_lr0.001</a></strong> to <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/ij9kejdj' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/ij9kejdj</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/ij9kejdj' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/sweeps/ij9kejdj</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/js51yip4' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/js51yip4</a>"},"metadata":{}},{"name":"stdout","text":"\nTraining attention-based transliteration model for language: hi\nUsing dataset from: /kaggle/input/dataset/dakshina_dataset_v1.0/hi/lexicons\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/20: 100%|██████████| 691/691 [00:25<00:00, 26.80it/s, loss=0.605]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Train Loss: 1.1289, Val Loss: 9.1297, Val Accuracy: 0.3079\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 100%|██████████| 691/691 [00:25<00:00, 26.71it/s, loss=0.35] \n\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Train Loss: 0.5801, Val Loss: 9.8530, Val Accuracy: 0.3508\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 100%|██████████| 691/691 [00:25<00:00, 27.02it/s, loss=0.458]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, Train Loss: 0.4709, Val Loss: 10.0881, Val Accuracy: 0.3814\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 100%|██████████| 691/691 [00:25<00:00, 26.87it/s, loss=0.368]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, Train Loss: 0.4158, Val Loss: 10.4205, Val Accuracy: 0.4011\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 100%|██████████| 691/691 [00:25<00:00, 26.85it/s, loss=0.376]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 5, Train Loss: 0.3711, Val Loss: 10.5983, Val Accuracy: 0.4039\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/20: 100%|██████████| 691/691 [00:25<00:00, 26.82it/s, loss=0.382]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 6, Train Loss: 0.3353, Val Loss: 11.0645, Val Accuracy: 0.4061\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/20: 100%|██████████| 691/691 [00:25<00:00, 27.04it/s, loss=0.27] \n","output_type":"stream"},{"name":"stdout","text":"Epoch: 7, Train Loss: 0.3095, Val Loss: 11.2805, Val Accuracy: 0.4078\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/20: 100%|██████████| 691/691 [00:25<00:00, 27.15it/s, loss=0.318]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 8, Train Loss: 0.2926, Val Loss: 11.3603, Val Accuracy: 0.4218\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/20: 100%|██████████| 691/691 [00:25<00:00, 27.06it/s, loss=0.261]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 9, Train Loss: 0.2686, Val Loss: 11.6661, Val Accuracy: 0.4224\nModel successfully saved to W&B as artifact\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/20: 100%|██████████| 691/691 [00:25<00:00, 26.88it/s, loss=0.228]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 10, Train Loss: 0.2491, Val Loss: 12.0183, Val Accuracy: 0.4112\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/20: 100%|██████████| 691/691 [00:25<00:00, 26.87it/s, loss=0.457]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 11, Train Loss: 0.2374, Val Loss: 12.1551, Val Accuracy: 0.4009\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/20: 100%|██████████| 691/691 [00:25<00:00, 26.88it/s, loss=0.208]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 12, Train Loss: 0.2266, Val Loss: 12.5110, Val Accuracy: 0.4002\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/20: 100%|██████████| 691/691 [00:25<00:00, 26.94it/s, loss=0.36] \n","output_type":"stream"},{"name":"stdout","text":"Epoch: 13, Train Loss: 0.2178, Val Loss: 12.7693, Val Accuracy: 0.4117\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/20: 100%|██████████| 691/691 [00:25<00:00, 26.82it/s, loss=0.381] \n","output_type":"stream"},{"name":"stdout","text":"Epoch: 14, Train Loss: 0.2061, Val Loss: 12.6726, Val Accuracy: 0.4050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/20: 100%|██████████| 691/691 [00:25<00:00, 27.06it/s, loss=0.154]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 15, Train Loss: 0.1988, Val Loss: 12.9655, Val Accuracy: 0.4036\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/20: 100%|██████████| 691/691 [00:25<00:00, 26.92it/s, loss=0.226]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 16, Train Loss: 0.1931, Val Loss: 13.2487, Val Accuracy: 0.4105\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/20: 100%|██████████| 691/691 [00:25<00:00, 26.98it/s, loss=0.21]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch: 17, Train Loss: 0.1921, Val Loss: 13.2448, Val Accuracy: 0.4009\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/20: 100%|██████████| 691/691 [00:25<00:00, 26.88it/s, loss=0.199] \n","output_type":"stream"},{"name":"stdout","text":"Epoch: 18, Train Loss: 0.1809, Val Loss: 13.5084, Val Accuracy: 0.4039\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/20: 100%|██████████| 691/691 [00:25<00:00, 26.79it/s, loss=0.306] \n","output_type":"stream"},{"name":"stdout","text":"Epoch: 19, Train Loss: 0.1742, Val Loss: 13.7959, Val Accuracy: 0.3922\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/20: 100%|██████████| 691/691 [00:25<00:00, 26.77it/s, loss=0.316] \n","output_type":"stream"},{"name":"stdout","text":"Epoch: 20, Train Loss: 0.1721, Val Loss: 13.9337, Val Accuracy: 0.3917\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▇▇▇▇██▇▇▇▇▇▇▇▇▇▆▆</td></tr><tr><td>val_loss</td><td>▁▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_accuracy</td><td>0.42244</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>language</td><td>hi</td></tr><tr><td>train_loss</td><td>0.17209</td></tr><tr><td>val_accuracy</td><td>0.39169</td></tr><tr><td>val_loss</td><td>13.93369</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">attn_hi_GRU_drop0.5_es64_hs512_en1_de1_lr0.001</strong> at: <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/js51yip4' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq/runs/js51yip4</a><br> View project at: <a href='https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq' target=\"_blank\">https://wandb.ai/da24m006-iit-madras/DL_A3_seq2seq</a><br>Synced 5 W&B file(s), 0 media file(s), 18 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_214504-js51yip4/logs</code>"},"metadata":{}}],"execution_count":17}]}